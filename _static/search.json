[
  {
    "objectID": "paper.html",
    "href": "paper.html",
    "title": "NSF-Soft-Search: Two Datasets to Study the Identification and Production of Research Software",
    "section": "",
    "text": "Software production, use, and reuse is an increasingly crucial part of scholarly work (Bhattarai, Ghassemi, and Alhanai 2022; Trisovic et al. 2021). While historically underutilized, citing and referencing software used during the course of research is becoming common with new standards for software citation (Katz et al. 2021; Du et al. 2022) and work in extracting software references in existing literature (Istrate et al. 2022). However, records of software production are not readily identifiable or available at scale in the way that peer-reviewed publications or other scholarly outputs are (Schindler et al. 2022). To make progress on this problem, we introduce two related datasets for studying and inferring software produced as a part of research, which we refer to as the NSF-Soft-Search dataset.\n\nBhattarai, Prajjwal, Mohammed Ghassemi, and Tuka Alhanai. 2022. “Open-Source Code Repository Attributes Predict Impact of Computer Science Research.” In Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries. JCDL ’22. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3529372.3530927.\n\nTrisovic, Ana, Matthew K. Lau, Thomas Pasquier, and Mercè Crosas. 2021. “A Large-Scale Study on Research Code Quality and Execution.” Scientific Data 9.\n\nKatz, Daniel S., Neil P. Chue Hong, Tim Clark, August Muench, Shelley Stall, Daina R. Bouquin, Matthew Cannon, et al. 2021. “Recognizing the Value of Software: A Software Citation Guide.” F1000Research 9.\n\nDu, Cai Fan, Johanna Cohoon, Patrice Lopez, and James Howison. 2022. “Understanding Progress in Software Citation: A Study of Software Citation in the CORD-19 Corpus.” PeerJ Computer Science 8.\n\nIstrate, Ana-Maria, Donghui Li, Dario Taraborelli, Michaela Torkar, Boris Veytsman, and Ivana Williams. 2022. “A Large Dataset of Software Mentions in the Biomedical Literature.” arXiv. https://doi.org/10.48550/ARXIV.2209.00693.\n\nSchindler, David, Felix Bensmann, Stefan Dietze, and Frank Krüger. 2022. “The Role of Software in Science: A Knowledge Graph-Based Analysis of Software Mentions in PubMed Central.” PeerJ Computer Science 8.\nThe NSF-Soft-Search dataset is aimed at identifying research projects which are likely to have produced software while funded by a federal grant. We start by identifying GitHub repositories that acknowledge funding from at least one National Science Foundation (NSF) award. We then annotate each GitHub repository found with a binary decision for its contents: software or not-software (e.g. not all github repositories contain software, they might include research notes, course materials, etc.). We then link each annotated GitHub repository to the specific NSF award ID(s) referenced in its README.md file. Finally, we compile the NSF-Soft-Search Training dataset using the annotations for each GitHub repository, and the text from the linked NSF award abstract and the project outcomes report.\nUsing the NSF-Soft-Search Training dataset, we train a variety of models to predict software production using either the NSF award abstract or project outcomes report text as input. We use the best performing models to then infer software production against all awards funded by the National Science Foundation from 2010 to 2023 (additional details are offered in Section 2). The predictions and metadata for each NSF award between the 2010 and 2023 period are compiled to form the NSF-Soft-Search Inferred dataset.\nIn total, our new NSF-Soft-Search dataset includes the following contributions:\n\nNSF-Soft-Search Training: A ground truth dataset compiled using linked NSF awards and GitHub repositories which have been annotated for software production.\nMultiple classifiers which infer software production from either the text of an NSF award’s abstract or project outcomes report.\nNSF-Soft-Search Inferred: A dataset of more than 150,000 NSF funded awards from between 2010 and 2023. Each award has two predictions for software production: one from prediction using the abstract text and the other from prediction using the project outcomes report text.\n\nThe rest of the paper proceeds as follows. In Section 2 we detail the data collection and annotation process used for creating the NSF-Soft-Search Training dataset. In Section 3 we briefly describe the model training process and report results. In Section 4 we provide summary statistics for the NSF-Soft-Search Inferred dataset and observe trends in software production over time. We conclude with discussion regarding the limitations of our approach and opportunities for future work."
  },
  {
    "objectID": "paper.html#sec-finding-soft",
    "href": "paper.html#sec-finding-soft",
    "title": "NSF-Soft-Search: Two Datasets to Study the Identification and Production of Research Software",
    "section": "2.1 Finding Software Produced by NSF Awards",
    "text": "2.1 Finding Software Produced by NSF Awards\nThe first step in our data collection process was to find software outputs from National Science Foundation (NSF) funded research. This step has two potential approaches. The first approach is a manual search for references and promises of software production within NSF award abstracts, project outcome reports, and papers supported by each award. This first approach is labor intensive and may be prone to labeling errors because while there may be a promise of software production in these documents, it may not be possible to verify such software was ultimately produced. The other approach is to predict software production using a trained model. We pursue this approach with the caveat that there are also potential label errors.\n\n\nTo gather examples of verifiable software production, we created a Python script which used the GitHub API to search for repositories which included reference to financial support from an NSF award in the repositories README.md file. Specifically our script queried for README.md files which contained any of the following text snippets: ‘National Science Foundation’, ‘NSF Award’, ‘NSF Grant’, ‘Supported by NSF’, or ‘Supported by the NSF’. GitHub was selected as the basis for our search because of its widespread adoption and mention in scholarly publication (Escamilla et al. 2022). This search found 1520 unique repositories which contained a reference to the NSF in the repository’s README.md file.\n\nEscamilla, Emily, Martin Klein, Talya Cooper, Vicky Rampin, Michele C. Weigle, and Michael L. Nelson. 2022. “The Rise of GitHub in Scholarly Publications.” arXiv. https://doi.org/10.48550/ARXIV.2208.04895."
  },
  {
    "objectID": "paper.html#software-production-annotation",
    "href": "paper.html#software-production-annotation",
    "title": "NSF-Soft-Search: Two Datasets to Study the Identification and Production of Research Software",
    "section": "2.2 Software Production Annotation",
    "text": "2.2 Software Production Annotation\nThe next step in our data collection process was to annotate each of the GitHub repositories found as either “software” or “not software.” In our initial review of the repositories we had collected, we found that the content of repositories ranged from documentation, experimental notes, course materials, collections of one-off scripts written during a research project, to more typical software libraries with installation instructions, testing, and community support and use.\nUsing existing definitions of what constitutes research software to form the basis of our annotation criteria (Martinez-Ortiz et al. 2022; Sochat et al. 2022), we conducted multiple rounds of trial coding on samples of the data. Fleiss’ kappa was used to determine if there was agreement between our research team on whether ten GitHub repositories contained ‘software’ or not. On each round of trial coding ten GitHub repositories were randomly selected from our dataset for each member of our research team to annotate independently. When assessing a repository, members of the research team were allowed to use any information in the repository to determine their annotation (i.e. the content of the README.md file, the repository activity, documentation availability, etc.)\n\nMartinez-Ortiz, Carlos, Paula Martinez Lavanchy, Laurents Sesink, Brett G. Olivier, James Meakin, Maaike de Jong, and Maria Cruz. 2022. “Practical Guide to Software Management Plans.” Zenodo. https://doi.org/10.5281/zenodo.7185371.\n\nSochat, Vanessa, Nicholas May, Ian Cosden, Carlos Martinez-Ortiz, and Sadie Bartholomew. 2022. “The Research Software Encyclopedia: A Community Framework to Define Research Software.” Journal of Open Research Software.\n\n\nOur final round of trial coding showed that there was near perfect agreement between the research team (K=0.892) (Viera, Garrett, et al. 2005).\n\nViera, Anthony J, Joanne M Garrett, et al. 2005. “Understanding Interobserver Agreement: The Kappa Statistic.” Fam Med 37 (5): 360–63.\n\n\nOur final annotation criteria was generally inclusive of labeling repositories as software, rather there were specific exclusion criteria that resulted in a repository being labeled as “not software”. Specifically repositories were labeled as “not software” when a repository primarily consisted of:\n\nproject documentation or research notes\nteaching materials for a workshop or course\nthe source code for a project or research lab website\ncollections of scripts specific to the analysis of a single experiment without regard to further generalizability\nutility functions for accessing data without providing any additional processing capacity\n\nWe then annotated all GitHub repositories in our dataset as either “software” or “not software” according to our agreed upon annotation criteria."
  },
  {
    "objectID": "paper.html#linking-github-repositories-to-nsf-awards",
    "href": "paper.html#linking-github-repositories-to-nsf-awards",
    "title": "NSF-Soft-Search: Two Datasets to Study the Identification and Production of Research Software",
    "section": "2.3 Linking GitHub Repositories to NSF Awards",
    "text": "2.3 Linking GitHub Repositories to NSF Awards\nOur final step in the data collection process was to link the annotated GitHub repositories back to specific NSF awards. To do so, we created a script which would load the webpage for each GitHub repository, scrape the content of the repository’s README and find the specific NSF award ID number(s) referenced. While annotating the dataset, and with this script, our dataset size was reduced as we found some repositories were returned in the initial search because of the “NSF” acronym being used by other, non-United-States governmental agencies which also fund research.\nWhen processing each repository, our Python script would load the README content, search for NSF Award ID patterns with regular expressions, and then verify that each NSF award ID found was valid by requesting metadata for the award from the NSF award API.\n\n\nWe then retrieved the text for each award’s abstract and project outcomes report. This was the final step of our data collection process and allowed us to create a dataset of 446 unique NSF awards labeled as ‘produced software’ and 471 unique NSF awards labeled as ‘did not produce software’."
  },
  {
    "objectID": "paper.html#trends-and-observations",
    "href": "paper.html#trends-and-observations",
    "title": "NSF-Soft-Search: Two Datasets to Study the Identification and Production of Research Software",
    "section": "4.1 Trends and Observations",
    "text": "4.1 Trends and Observations\n\n\n\n\n\nFigure 1: Software Production Over Time (Using Predictions from Abstracts)\n\n\n\n\nUsing the NSF-Soft-Search Inferred dataset we can observe trends in software production over time. Figure 1 plots the percent of awards which we predict to have produced software (using the award’s abstract) over time. While there are minor year-to-year deviations in predicted software production, we observe the “Math and Physical Sciences” (MPS) funding program as funding the most awards which we predict to produce software, with “Computer and Information Science and Engineering” (CISE), and “Engineering” (ENG) close behind.\n\n\n\n\n\nFigure 2: Software Production Grouped By Award Duration (Using Predictions from Abstracts)\n\n\n\n\nWe can additionally observe trends in software production as award duration increases. Figure 2 plots the percent of awards which we predict to have produced software (using the award’s abstract) grouped by the award duration in years. We note that as award duration increases, the percentage of awards which are predicted to have produced software also tends to increase."
  },
  {
    "objectID": "paper.html#limitations",
    "href": "paper.html#limitations",
    "title": "NSF-Soft-Search: Two Datasets to Study the Identification and Production of Research Software",
    "section": "5.1 Limitations",
    "text": "5.1 Limitations\nAs discussed in Section 2, the NSF-Soft-Search Training dataset was entirely composed of NSF awards which ultimately released or hosted software (and other research products) on GitHub. Due to our data collection strategy, it is possible that each of the predictive models learned not to predict if an NSF award would produce software, but rather, if an NSF award would produce software hosted on GitHub."
  },
  {
    "objectID": "paper.html#future-work",
    "href": "paper.html#future-work",
    "title": "NSF-Soft-Search: Two Datasets to Study the Identification and Production of Research Software",
    "section": "5.2 Future Work",
    "text": "5.2 Future Work\nAs discussed in Section 2.1, our initial method for attempting to find research software produced from NSF supported awards was to search for references and promises of software production in the abstract, project outcomes report, and attached papers of each award. While attempting this approach to create the dataset, we found that many awards and papers that reference computational methods do not provide a reference web link to their code repositories or websites. In some cases, we found repositories related to an award or paper via Google and GitHub search ourselves. While we support including references to code repositories in award abstracts, outcomes reports, and papers, future research should be conducted on how to enable automatic reconnection of papers and their software outputs."
  }
]