{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54b23719-9b85-4351-be50-c72b67ecc6a4",
   "metadata": {},
   "source": [
    "# RSE GitHub Repo Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937704ea-b67a-40f0-800d-9ab2f3802683",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "Load the annotated dataset used for training from the project, then use the `github_link` column to threaded request the README content.\n",
    "Store to a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4307916-9b72-429c-8f4f-02f574155028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from soft_search.data import load_soft_search_2022_training\n",
    "\n",
    "df = load_soft_search_2022_training()\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59277ae-1a0a-42fb-afbe-f17b2c990fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import HTTPError\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "def _process_row(row):\n",
    "    # Request GitHub page\n",
    "    try:\n",
    "        github_page = requests.get(row.github_link)\n",
    "        github_page.raise_for_status()\n",
    "    except HTTPError:\n",
    "        row[\"readme_text\"] = None\n",
    "        return row\n",
    "\n",
    "    # Find and scrape README text\n",
    "    soup = BeautifulSoup(github_page.content, \"html.parser\")\n",
    "    readme_container = soup.find(id=\"readme\")\n",
    "    if readme_container is None:\n",
    "        row[\"readme_text\"] = \"no readme available\"\n",
    "        return row\n",
    "\n",
    "    # Add the readme text to the row\n",
    "    row[\"readme_text\"] = readme_container.text\n",
    "    time.sleep(randint(0, 5))\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac3a71-b1a0-44e1-83c3-053bf6ad7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = thread_map(_process_row, [row for _, row in df.iterrows()], total=len(df))\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_parquet(\"github-readme-with-software-prediction.parquet\")\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24393f4f-7973-49ce-b8da-0c1404450f8c",
   "metadata": {},
   "source": [
    "## Modeling Training\n",
    "\n",
    "I wrote a package called `lazy-text-classifiers` a while back that let's me train a bunch of differents models in a single go and see what works best.\n",
    "\n",
    "Installing locally here though just for minor additions and changes (adding and removing certain models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9fec8d-45d6-4277-8124-74315f75e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ../../personal/lazy-text-classifiers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd55c0e7-3b15-4c85-811e-c6686f6edd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>github_link</th>\n",
       "      <th>nsf_award_id</th>\n",
       "      <th>nsf_award_link</th>\n",
       "      <th>from_template_repo</th>\n",
       "      <th>is_a_fork</th>\n",
       "      <th>abstract_text</th>\n",
       "      <th>project_outcomes</th>\n",
       "      <th>readme_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>software-predicted</td>\n",
       "      <td>https://github.com/junyanz/VON</td>\n",
       "      <td>1524817</td>\n",
       "      <td>https://www.nsf.gov/awardsearch/showAward?AWD_...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>The goal of this work is to develop a set of t...</td>\n",
       "      <td>It is an exciting time for computer vision. Wi...</td>\n",
       "      <td>Visual Object Networks\\nExample results\\nMore ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>software-not-predicted</td>\n",
       "      <td>https://github.com/sugwg/sn-core-bounce-pe</td>\n",
       "      <td>1836702</td>\n",
       "      <td>https://www.nsf.gov/awardsearch/showAward?AWD_...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>The NSF's Advanced LIGO and the European Virgo...</td>\n",
       "      <td>Albert Einstein predicted gravitational waves,...</td>\n",
       "      <td>Inferring physical properties of stellar colla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>software-predicted</td>\n",
       "      <td>https://github.com/IBPA/SBROME</td>\n",
       "      <td>1146926</td>\n",
       "      <td>https://www.nsf.gov/awardsearch/showAward?AWD_...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Synthetic Biology is a nascent field with appl...</td>\n",
       "      <td>The design of biological circuits requires to ...</td>\n",
       "      <td>SBROME\\nWhat is SBROME?\\nDependencies\\nInstall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       label                                 github_link  \\\n",
       "1377      software-predicted              https://github.com/junyanz/VON   \n",
       "115   software-not-predicted  https://github.com/sugwg/sn-core-bounce-pe   \n",
       "977       software-predicted              https://github.com/IBPA/SBROME   \n",
       "\n",
       "     nsf_award_id                                     nsf_award_link  \\\n",
       "1377      1524817  https://www.nsf.gov/awardsearch/showAward?AWD_...   \n",
       "115       1836702  https://www.nsf.gov/awardsearch/showAward?AWD_...   \n",
       "977       1146926  https://www.nsf.gov/awardsearch/showAward?AWD_...   \n",
       "\n",
       "      from_template_repo  is_a_fork  \\\n",
       "1377               False      False   \n",
       "115                False      False   \n",
       "977                False      False   \n",
       "\n",
       "                                          abstract_text  \\\n",
       "1377  The goal of this work is to develop a set of t...   \n",
       "115   The NSF's Advanced LIGO and the European Virgo...   \n",
       "977   Synthetic Biology is a nascent field with appl...   \n",
       "\n",
       "                                       project_outcomes  \\\n",
       "1377  It is an exciting time for computer vision. Wi...   \n",
       "115   Albert Einstein predicted gravitational waves,...   \n",
       "977   The design of biological circuits requires to ...   \n",
       "\n",
       "                                            readme_text  \n",
       "1377  Visual Object Networks\\nExample results\\nMore ...  \n",
       "115   Inferring physical properties of stellar colla...  \n",
       "977   SBROME\\nWhat is SBROME?\\nDependencies\\nInstall...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"github-readme-with-software-prediction.parquet\")\n",
    "df = df.dropna(subset=[\"label\", \"readme_text\"])\n",
    "df.readme_text = df.readme_text.str.strip()\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "694cb42b-a948-4ef8-830e-71247a1fc807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732 184 732 184\n",
      "Initializing model: 'tfidf-logit'\n",
      "Fitting model: 'tfidf-logit'\n",
      "Testing model: 'tfidf-logit'\n",
      "'tfidf-logit' eval results: {'model': 'tfidf-logit', 'accuracy': 0.8858695652173914, 'balanced_accuracy': 0.8864285714285715, 'precision': 0.8866056662390455, 'recall': 0.8858695652173914, 'f1': 0.8860016959193393, 'time': 23.11274494600002}\n",
      "Initializing model: 'semantic-logit-distilbert-sst2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/eva/.cache/torch/sentence_transformers/distilbert-base-uncased-finetuned-sst-2-english. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model: 'semantic-logit-distilbert-sst2'\n",
      "Testing model: 'semantic-logit-distilbert-sst2'\n",
      "'semantic-logit-distilbert-sst2' eval results: {'model': 'semantic-logit-distilbert-sst2', 'accuracy': 0.8206521739130435, 'balanced_accuracy': 0.8197619047619047, 'precision': 0.8208607817303468, 'recall': 0.8206521739130435, 'f1': 0.8207321661045128, 'time': 24.197274980000657}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf-logit</td>\n",
       "      <td>0.885870</td>\n",
       "      <td>0.886429</td>\n",
       "      <td>0.886606</td>\n",
       "      <td>0.885870</td>\n",
       "      <td>0.886002</td>\n",
       "      <td>23.112745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>semantic-logit-distilbert-sst2</td>\n",
       "      <td>0.820652</td>\n",
       "      <td>0.819762</td>\n",
       "      <td>0.820861</td>\n",
       "      <td>0.820652</td>\n",
       "      <td>0.820732</td>\n",
       "      <td>24.197275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model  accuracy  balanced_accuracy  precision  \\\n",
       "0                     tfidf-logit  0.885870           0.886429   0.886606   \n",
       "1  semantic-logit-distilbert-sst2  0.820652           0.819762   0.820861   \n",
       "\n",
       "     recall        f1       time  \n",
       "0  0.885870  0.886002  23.112745  \n",
       "1  0.820652  0.820732  24.197275  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lazy_text_classifiers import LazyTextClassifiers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(20220420)\n",
    "np.random.seed(20220420)\n",
    "\n",
    "# Example data from sklearn\n",
    "# `x` should be an iterable of strings\n",
    "# `y` should be an iterable of string labels\n",
    "x = df.readme_text\n",
    "y = df.label\n",
    "\n",
    "# Split the data into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=20220420,\n",
    ")\n",
    "\n",
    "print(len(x_train), len(x_test), len(y_train), len(y_test))\n",
    "\n",
    "# Init and fit all models\n",
    "ltc = LazyTextClassifiers(random_state=12)\n",
    "results = ltc.fit(x_train, x_test, y_train, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4169452-56ba-4aae-b5e9-d706c2c04a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_logit = ltc.fit_models[\"semantic-logit-distilbert-sst2\"]\n",
    "tfidf_logit = ltc.fit_models[\"tfidf-logit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec672b-d615-4652-9f16-d14ffcf3f3fe",
   "metadata": {},
   "source": [
    "## Eval on Repos I Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a002f04-ae64-4205-b31e-2f1433c15ed7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "SOFT_SEARCH_README = \"\"\"\n",
    "# soft-search\n",
    "\n",
    "[![Build Status](https://github.com/si2-urssi/eager/workflows/CI/badge.svg)](https://github.com/si2-urssi/eager/actions)\n",
    "[![Documentation](https://github.com/si2-urssi/eager/workflows/Documentation/badge.svg)](https://si2-urssi.github.io/eager)\n",
    "\n",
    "searching for software promises in grant applications\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "**Stable Release:** `pip install soft-search`<br>\n",
    "**Development Head:** `pip install git+https://github.com/si2-urssi/eager.git`\n",
    "\n",
    "This repository contains the library code and the paper generation code\n",
    "created for our paper [Searching for Software in NSF Awards](https://si2-urssi.github.io/eager/_static/paper.html).\n",
    "\n",
    "### Abstract\n",
    "Software is an important tool for scholarly work, but software produced for research is in many cases not easily identifiable or discoverable. A potential first step in linking research and software is software identification. In this paper we present two datasets to study the identification and production of research software. The first dataset contains almost 1000 human labeled annotations of software production from National Science Foundation (NSF) awarded research projects. We use this dataset to train models that  predict software production. Our second dataset is created by applying the trained predictive models across the abstracts and project outcomes reports for all NSF funded projects between the years of 2010 and 2023. The result is an inferred dataset of software production for over 150,000 NSF awards. We release the Soft-Search dataset to aid in identifying and understanding research software production: https://github.com/si2-urssi/eager\n",
    "\n",
    "\n",
    "## The Soft-Search Inferred Dataset\n",
    "\n",
    "Please download the 500MB Soft-Search Inferred dataset from\n",
    "[Google Drive](https://drive.google.com/file/d/1k0jvs47bCWT18GHOMXY6EdG5MIDdCiM2/view?usp=share_link).\n",
    "\n",
    "The dataset is shared as a `parquet` file and can be read in Python with\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "nsf_soft_search = pd.read_parquet(\"nsf-soft-search-2022.parquet\")\n",
    "```\n",
    "\n",
    "Please view the\n",
    "[Parquet R Documentation](https://arrow.apache.org/docs/r/reference/read_parquet.html)\n",
    "for information regarding reading the dataset in R.\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "1. Load our best model (the \"TF-IDF Vectorizer Logistic Regression Model\")\n",
    "2. Pull award abstract texts from the NSF API\n",
    "3. Predict if the award will produce software using the abstract text for each award\n",
    "\n",
    "```python\n",
    "from soft_search.constants import NSFFields, NSFPrograms\n",
    "from soft_search.label import (\n",
    "  load_tfidf_logit_for_prediction_from_abstract,\n",
    "  load_tfidf_logit_for_prediction_from_outcomes,\n",
    ")\n",
    "from soft_search.nsf import get_nsf_dataset\n",
    "\n",
    "# Load the abstract model\n",
    "pipeline = load_tfidf_logit_for_prediction_from_abstract()\n",
    "# or load the outcomes model\n",
    "# pipeline = load_tfidf_logit_for_prediction_from_outcomes()\n",
    "\n",
    "# Pull data\n",
    "data = get_nsf_dataset(\n",
    "  start_date=\"2022-10-01\",\n",
    "  end_date=\"2023-01-01\",\n",
    "  program_name=NSFPrograms.Mathematical_and_Physical_Sciences,\n",
    "  dataset_fields=[\n",
    "    NSFFields.id_,\n",
    "    NSFFields.abstractText,\n",
    "    NSFFields.projectOutComesReport,\n",
    "  ],\n",
    "  require_project_outcomes_doc=False,\n",
    ")\n",
    "\n",
    "# Predict\n",
    "data[\"prediction_from_abstract\"] = pipeline.predict(data[NSFFields.abstractText])\n",
    "print(data[[\"id\", \"prediction_from_abstract\"]])\n",
    "\n",
    "#           id prediction_from_abstract\n",
    "# 0    2238468   software-not-predicted\n",
    "# 1    2239561   software-not-predicted\n",
    "```\n",
    "\n",
    "### Annotated Training Data\n",
    "\n",
    "```python\n",
    "from soft_search.data import load_soft_search_2022_training\n",
    "\n",
    "df = load_soft_search_2022_training()\n",
    "```\n",
    "\n",
    "### Reproducible Models\n",
    "\n",
    "| predictive_source \t| model                  \t| accuracy \t| precision \t| recall   \t| f1       \t|\n",
    "|-------------------\t|------------------------\t|----------\t|-----------\t|----------\t|----------\t|\n",
    "| project-outcomes  \t| tfidf-logit            \t| 0.744898 \t| 0.745106  \t| 0.744898 \t| 0.744925 \t|\n",
    "| project-outcomes  \t| fine-tuned-transformer \t| 0.673469 \t| 0.637931  \t| 0.770833 \t| 0.698113 \t|\n",
    "| abstract-text     \t| tfidf-logit            \t| 0.673913 \t| 0.673960  \t| 0.673913 \t| 0.673217 \t|\n",
    "| abstract-text     \t| fine-tuned-transformer \t| 0.635870 \t| 0.607843  \t| 0.696629 \t| 0.649215 \t|\n",
    "| project-outcomes  \t| semantic-logit         \t| 0.632653 \t| 0.632568  \t| 0.632653 \t| 0.632347 \t|\n",
    "| abstract-text     \t| semantic-logit         \t| 0.630435 \t| 0.630156  \t| 0.630435 \t| 0.629997 \t|\n",
    "| abstract-text     \t| regex                  \t| 0.516304 \t| 0.514612  \t| 0.516304 \t| 0.513610 \t|\n",
    "| project-outcomes  \t| regex                  \t| 0.510204 \t| 0.507086  \t| 0.510204 \t| 0.481559 \t|\n",
    "\n",
    "To train and evaluate all of our models you can run the following:\n",
    "\n",
    "```bash\n",
    "pip install soft-search\n",
    "\n",
    "fit-and-eval-all-models\n",
    "```\n",
    "\n",
    "Also available directly in Python\n",
    "\n",
    "```python\n",
    "from soft_search.label.model_selection import fit_and_eval_all_models\n",
    "\n",
    "results = fit_and_eval_all_models()\n",
    "```\n",
    "\n",
    "## Annotated Dataset Creation\n",
    "\n",
    "1. We queried GitHub for repositories with references to NSF Awards.\n",
    "  - We specifically queried for the terms: \"National Science Foundation\", \"NSF Award\",\n",
    "    \"NSF Grant\", \"Supported by the NSF\", and \"Supported by NSF\". This script is available\n",
    "    with the command `get-github-repositories-with-nsf-ref`. The code for the script is\n",
    "    available at the following link:\n",
    "    https://github.com/si2-urssi/eager/blob/main/soft_search/bin/get_github_repositories_with_nsf_ref.py\n",
    "  - Note: the `get-github-repositories-with-nsf-ref` script produces a directory of CSV\n",
    "    files. This is useful for paginated queries and protecting against potential crashes\n",
    "    but the actual stored data in the repo (and the data we use going forward) is\n",
    "    the a DataFrame with all of these chunks concatenated together and duplicate GitHub\n",
    "    repositories removed.\n",
    "  - Because the `get-github-repositories-with-nsf-ref` script depends on the returned\n",
    "    data from GitHub themselves, we have archived the data produced by the original run\n",
    "    of this script to the repository and made it available as follows:\n",
    "    ```python\n",
    "    from soft_search.data import load_github_repos_with_nsf_refs_2022\n",
    "\n",
    "    data = load_github_repos_with_nsf_refs_2022()\n",
    "    ```\n",
    "2. We manually labeled each of the discovered repositories as \"software\"\n",
    "   or \"not software\" and cleaned up the dataset to only include awards \n",
    "   which have a valid NSF Award ID.\n",
    "  - A script was written to find all NSF Award IDs within a repositories README.md file\n",
    "    and check that each NSF Award ID found was valid (if we could successfully query\n",
    "    that award ID using the NSF API). Only valid NSF Award IDs were kept and therefore,\n",
    "    only GitHub repositories which contained valid NSF Award IDs were kept in the\n",
    "    dataset. This script is available with the command\n",
    "    `find-nsf-award-ids-in-github-readmes-and-link`. The code for the script is\n",
    "    available at the following link:\n",
    "    https://github.com/si2-urssi/eager/blob/main/soft_search/bin/find_nsf_award_ids_in_github_readmes_and_link.py\n",
    "  - A function was written to merge all of the manual annotations and the NSF Award IDs\n",
    "    found. This function also stored the cleaned and prepared data to the project data\n",
    "    directory. The code for this function is available at the following link:\n",
    "    https://github.com/si2-urssi/eager/blob/main/soft_search/data/soft_search_2022.py#L143\n",
    "  - The manually labeled, cleaned, prepared, and stored data is made available with the\n",
    "    following code:\n",
    "     ```python\n",
    "     from soft_search.data import load_soft_search_2022_training\n",
    "\n",
    "     data = load_soft_search_2022_training()\n",
    "     ```\n",
    "  - Prior to the manual annotation process, we conducted multiple rounds of\n",
    "    annotation trials to ensure we had agreement on our labeling definitions.\n",
    "    The final annotation trial results which resulted in an inter-rater\n",
    "    reliability (Fleiss Kappa score) of 0.8918 (near perfect) is available\n",
    "    via the following function:\n",
    "    ```python\n",
    "    from soft_search.data import load_soft_search_2022_training_irr\n",
    "\n",
    "    data = load_soft_search_2022_training_irr()\n",
    "    ```\n",
    "    Additionally, the code for calculating the Fleiss Kappa Statistic\n",
    "    is available at the following link:\n",
    "    https://github.com/si2-urssi/eager/blob/main/soft_search/data/irr.py\n",
    "\n",
    "\n",
    "## Documentation\n",
    "\n",
    "For full package documentation please visit [si2-urssi.github.io/eager](https://si2-urssi.github.io/eager).\n",
    "\n",
    "## Development\n",
    "\n",
    "See [CONTRIBUTING.md](CONTRIBUTING.md) for information related to developing the code.\n",
    "\n",
    "**MIT License**\n",
    "\"\"\"\n",
    "\n",
    "AICSIMAGEIO_README = \"\"\"\n",
    "# AICSImageIO\n",
    "\n",
    "[![Build Status](https://github.com/AllenCellModeling/aicsimageio/workflows/Build%20Main/badge.svg)](https://github.com/AllenCellModeling/aicsimageio/actions)\n",
    "[![Documentation](https://github.com/AllenCellModeling/aicsimageio/workflows/Documentation/badge.svg)](https://AllenCellModeling.github.io/aicsimageio/)\n",
    "[![Code Coverage](https://codecov.io/gh/AllenCellModeling/aicsimageio/branch/main/graph/badge.svg)](https://app.codecov.io/gh/AllenCellModeling/aicsimageio/branch/main)\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4906608.svg)](https://doi.org/10.5281/zenodo.4906608)\n",
    "\n",
    "Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Pure Python\n",
    "\n",
    "---\n",
    "\n",
    "## Features\n",
    "\n",
    "-   Supports reading metadata and imaging data for:\n",
    "    -   `OME-TIFF`\n",
    "    -   `TIFF`\n",
    "    -   `ND2` -- (`pip install aicsimageio[nd2]`)\n",
    "    -   `DV` -- (`pip install aicsimageio[dv]`)\n",
    "    -   `CZI` -- (`pip install aicspylibczi>=3.1.1 fsspec>=2022.8.0`)\n",
    "    -   `LIF` -- (`pip install readlif>=0.6.4`)\n",
    "    -   `PNG`, `GIF`, [etc.](https://github.com/imageio/imageio) -- (`pip install aicsimageio[base-imageio]`)\n",
    "    -   Files supported by [Bio-Formats](https://docs.openmicroscopy.org/bio-formats/latest/supported-formats.html) -- (`pip install aicsimageio bioformats_jar`) (Note: requires `java` and `maven`, see below for details.)\n",
    "-   Supports writing metadata and imaging data for:\n",
    "    -   `OME-TIFF`\n",
    "    -   `PNG`, `GIF`, [etc.](https://github.com/imageio/imageio) -- (`pip install aicsimageio[base-imageio]`)\n",
    "-   Supports reading and writing to\n",
    "    [fsspec](https://github.com/intake/filesystem_spec) supported file systems\n",
    "    wherever possible:\n",
    "\n",
    "    -   Local paths (i.e. `my-file.png`)\n",
    "    -   HTTP URLs (i.e. `https://my-domain.com/my-file.png`)\n",
    "    -   [s3fs](https://github.com/dask/s3fs) (i.e. `s3://my-bucket/my-file.png`)\n",
    "    -   [gcsfs](https://github.com/dask/gcsfs) (i.e. `gcs://my-bucket/my-file.png`)\n",
    "\n",
    "    See [Cloud IO Support](#cloud-io-support) for more details.\n",
    "\n",
    "## Installation\n",
    "\n",
    "**Stable Release:** `pip install aicsimageio`<br>\n",
    "**Development Head:** `pip install git+https://github.com/AllenCellModeling/aicsimageio.git`\n",
    "\n",
    "AICSImageIO is supported on Windows, Mac, and Ubuntu.\n",
    "For other platforms, you will likely need to build from source.\n",
    "\n",
    "#### Extra Format Installation\n",
    "\n",
    "TIFF and OME-TIFF reading and writing is always available after\n",
    "installing `aicsimageio`, but extra supported formats can be\n",
    "optionally installed using `[...]` syntax.\n",
    "\n",
    "-   For a single additional supported format (e.g. ND2): `pip install aicsimageio[nd2]`\n",
    "-   For a single additional supported format (e.g. ND2), development head: `pip install \"aicsimageio[nd2] @ git+https://github.com/AllenCellModeling/aicsimageio.git\"`\n",
    "-   For a single additional supported format (e.g. ND2), specific tag (e.g. `v4.0.0.dev6`): `pip install \"aicsimageio[nd2] @ git+https://github.com/AllenCellModeling/aicsimageio.git@v4.0.0.dev6\"`\n",
    "-   For faster OME-TIFF reading with tile tags: `pip install aicsimageio[bfio]`\n",
    "-   For multiple additional supported formats: `pip install aicsimageio[base-imageio,nd2]`\n",
    "-   For all additional supported (and openly licensed) formats: `pip install aicsimageio[all]`\n",
    "-   Due to the GPL license, LIF support is not included with the `[all]` extra, and must be installed manually with `pip install aicsimageio readlif>=0.6.4`\n",
    "-   Due to the GPL license, CZI support is not included with the `[all]` extra, and must be installed manually with `pip install aicsimageio aicspylibczi>=3.1.1 fsspec>=2022.8.0`\n",
    "-   Due to the GPL license, Bio-Formats support is not included with the `[all]` extra, and must be installed manually with `pip install aicsimageio bioformats_jar`. **Important!!** Bio-Formats support also requires a `java` and `mvn` executable in the environment. The simplest method is to install `bioformats_jar` from conda: `conda install -c conda-forge bioformats_jar` (which will additionally bring `openjdk` and `maven` packages).\n",
    "\n",
    "## Documentation\n",
    "\n",
    "For full package documentation please visit\n",
    "[allencellmodeling.github.io/aicsimageio](https://allencellmodeling.github.io/aicsimageio/index.html).\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "### Full Image Reading\n",
    "\n",
    "If your image fits in memory:\n",
    "\n",
    "```python\n",
    "from aicsimageio import AICSImage\n",
    "\n",
    "# Get an AICSImage object\n",
    "img = AICSImage(\"my_file.tiff\")  # selects the first scene found\n",
    "img.data  # returns 5D TCZYX numpy array\n",
    "img.xarray_data  # returns 5D TCZYX xarray data array backed by numpy\n",
    "img.dims  # returns a Dimensions object\n",
    "img.dims.order  # returns string \"TCZYX\"\n",
    "img.dims.X  # returns size of X dimension\n",
    "img.shape  # returns tuple of dimension sizes in TCZYX order\n",
    "img.get_image_data(\"CZYX\", T=0)  # returns 4D CZYX numpy array\n",
    "\n",
    "# Get the id of the current operating scene\n",
    "img.current_scene\n",
    "\n",
    "# Get a list valid scene ids\n",
    "img.scenes\n",
    "\n",
    "# Change scene using name\n",
    "img.set_scene(\"Image:1\")\n",
    "# Or by scene index\n",
    "img.set_scene(1)\n",
    "\n",
    "# Use the same operations on a different scene\n",
    "# ...\n",
    "```\n",
    "\n",
    "#### Full Image Reading Notes\n",
    "\n",
    "The `.data` and `.xarray_data` properties will load the whole scene into memory.\n",
    "The `.get_image_data` function will load the whole scene into memory and then retrieve\n",
    "the specified chunk.\n",
    "\n",
    "### Delayed Image Reading\n",
    "\n",
    "If your image doesn't fit in memory:\n",
    "\n",
    "```python\n",
    "from aicsimageio import AICSImage\n",
    "\n",
    "# Get an AICSImage object\n",
    "img = AICSImage(\"my_file.tiff\")  # selects the first scene found\n",
    "img.dask_data  # returns 5D TCZYX dask array\n",
    "img.xarray_dask_data  # returns 5D TCZYX xarray data array backed by dask array\n",
    "img.dims  # returns a Dimensions object\n",
    "img.dims.order  # returns string \"TCZYX\"\n",
    "img.dims.X  # returns size of X dimension\n",
    "img.shape  # returns tuple of dimension sizes in TCZYX order\n",
    "\n",
    "# Pull only a specific chunk in-memory\n",
    "lazy_t0 = img.get_image_dask_data(\"CZYX\", T=0)  # returns out-of-memory 4D dask array\n",
    "t0 = lazy_t0.compute()  # returns in-memory 4D numpy array\n",
    "\n",
    "# Get the id of the current operating scene\n",
    "img.current_scene\n",
    "\n",
    "# Get a list valid scene ids\n",
    "img.scenes\n",
    "\n",
    "# Change scene using name\n",
    "img.set_scene(\"Image:1\")\n",
    "# Or by scene index\n",
    "img.set_scene(1)\n",
    "\n",
    "# Use the same operations on a different scene\n",
    "# ...\n",
    "```\n",
    "\n",
    "#### Delayed Image Reading Notes\n",
    "\n",
    "The `.dask_data` and `.xarray_dask_data` properties and the `.get_image_dask_data`\n",
    "function will not load any piece of the imaging data into memory until you specifically\n",
    "call `.compute` on the returned Dask array. In doing so, you will only then load the\n",
    "selected chunk in-memory.\n",
    "\n",
    "### Mosaic Image Reading\n",
    "\n",
    "Read stitched data or single tiles as a dimension.\n",
    "\n",
    "Readers that support mosaic tile stitching:\n",
    "\n",
    "-   `LifReader`\n",
    "-   `CziReader`\n",
    "\n",
    "#### AICSImage\n",
    "\n",
    "If the file format reader supports stitching mosaic tiles together, the\n",
    "`AICSImage` object will default to stitching the tiles back together.\n",
    "\n",
    "```python\n",
    "img = AICSImage(\"very-large-mosaic.lif\")\n",
    "img.dims.order  # T, C, Z, big Y, big X, (S optional)\n",
    "img.dask_data  # Dask chunks fall on tile boundaries, pull YX chunks out of the image\n",
    "```\n",
    "\n",
    "This behavior can be manually turned off:\n",
    "\n",
    "```python\n",
    "img = AICSImage(\"very-large-mosaic.lif\", reconstruct_mosaic=False)\n",
    "img.dims.order  # M (tile index), T, C, Z, small Y, small X, (S optional)\n",
    "img.dask_data  # Chunks use normal ZYX\n",
    "```\n",
    "\n",
    "If the reader does not support stitching tiles together the M tile index will be\n",
    "available on the `AICSImage` object:\n",
    "\n",
    "```python\n",
    "img = AICSImage(\"some-unsupported-mosaic-stitching-format.ext\")\n",
    "img.dims.order  # M (tile index), T, C, Z, small Y, small X, (S optional)\n",
    "img.dask_data  # Chunks use normal ZYX\n",
    "```\n",
    "\n",
    "#### Reader\n",
    "\n",
    "If the file format reader detects mosaic tiles in the image, the `Reader` object\n",
    "will store the tiles as a dimension.\n",
    "\n",
    "If tile stitching is implemented, the `Reader` can also return the stitched image.\n",
    "\n",
    "```python\n",
    "reader = LifReader(\"ver-large-mosaic.lif\")\n",
    "reader.dims.order  # M, T, C, Z, tile size Y, tile size X, (S optional)\n",
    "reader.dask_data  # normal operations, can use M dimension to select individual tiles\n",
    "reader.mosaic_dask_data  # returns stitched mosaic - T, C, Z, big Y, big, X, (S optional)\n",
    "```\n",
    "\n",
    "#### Single Tile Absolute Positioning\n",
    "\n",
    "There are functions available on both the `AICSImage` and `Reader` objects\n",
    "to help with single tile positioning:\n",
    "\n",
    "```python\n",
    "img = AICSImage(\"very-large-mosaic.lif\")\n",
    "img.mosaic_tile_dims  # Returns a Dimensions object with just Y and X dim sizes\n",
    "img.mosaic_tile_dims.Y  # 512 (for example)\n",
    "\n",
    "# Get the tile start indices (top left corner of tile)\n",
    "y_start_index, x_start_index = img.get_mosaic_tile_position(12)\n",
    "```\n",
    "\n",
    "### Metadata Reading\n",
    "\n",
    "```python\n",
    "from aicsimageio import AICSImage\n",
    "\n",
    "# Get an AICSImage object\n",
    "img = AICSImage(\"my_file.tiff\")  # selects the first scene found\n",
    "img.metadata  # returns the metadata object for this file format (XML, JSON, etc.)\n",
    "img.channel_names  # returns a list of string channel names found in the metadata\n",
    "img.physical_pixel_sizes.Z  # returns the Z dimension pixel size as found in the metadata\n",
    "img.physical_pixel_sizes.Y  # returns the Y dimension pixel size as found in the metadata\n",
    "img.physical_pixel_sizes.X  # returns the X dimension pixel size as found in the metadata\n",
    "```\n",
    "\n",
    "### Xarray Coordinate Plane Attachment\n",
    "\n",
    "If `aicsimageio` finds coordinate information for the spatial-temporal dimensions of\n",
    "the image in metadata, you can use\n",
    "[xarray](http://xarray.pydata.org/en/stable/index.html) for indexing by coordinates.\n",
    "\n",
    "```python\n",
    "from aicsimageio import AICSImage\n",
    "\n",
    "# Get an AICSImage object\n",
    "img = AICSImage(\"my_file.ome.tiff\")\n",
    "\n",
    "# Get the first ten seconds (not frames)\n",
    "first_ten_seconds = img.xarray_data.loc[:10]  # returns an xarray.DataArray\n",
    "\n",
    "# Get the first ten major units (usually micrometers, not indices) in Z\n",
    "first_ten_mm_in_z = img.xarray_data.loc[:, :, :10]\n",
    "\n",
    "# Get the first ten major units (usually micrometers, not indices) in Y\n",
    "first_ten_mm_in_y = img.xarray_data.loc[:, :, :, :10]\n",
    "\n",
    "# Get the first ten major units (usually micrometers, not indices) in X\n",
    "first_ten_mm_in_x = img.xarray_data.loc[:, :, :, :, :10]\n",
    "```\n",
    "\n",
    "See `xarray`\n",
    "[\"Indexing and Selecting Data\" Documentation](http://xarray.pydata.org/en/stable/indexing.html)\n",
    "for more information.\n",
    "\n",
    "### Cloud IO Support\n",
    "\n",
    "[File-System Specification (fsspec)](https://github.com/intake/filesystem_spec) allows\n",
    "for common object storage services (S3, GCS, etc.) to act like normal filesystems by\n",
    "following the same base specification across them all. AICSImageIO utilizes this\n",
    "standard specification to make it possible to read directly from remote resources when\n",
    "the specification is installed.\n",
    "\n",
    "```python\n",
    "from aicsimageio import AICSImage\n",
    "\n",
    "# Get an AICSImage object\n",
    "img = AICSImage(\"http://my-website.com/my_file.tiff\")\n",
    "img = AICSImage(\"s3://my-bucket/my_file.tiff\")\n",
    "img = AICSImage(\"gcs://my-bucket/my_file.tiff\")\n",
    "\n",
    "# Or read with specific filesystem creation arguments\n",
    "img = AICSImage(\"s3://my-bucket/my_file.tiff\", fs_kwargs=dict(anon=True))\n",
    "img = AICSImage(\"gcs://my-bucket/my_file.tiff\", fs_kwargs=dict(anon=True))\n",
    "\n",
    "# All other normal operations work just fine\n",
    "```\n",
    "\n",
    "Remote reading requires that the file-system specification implementation for the\n",
    "target backend is installed.\n",
    "\n",
    "-   For `s3`: `pip install s3fs`\n",
    "-   For `gs`: `pip install gcsfs`\n",
    "\n",
    "See the [list of known implementations](https://filesystem-spec.readthedocs.io/en/latest/?badge=latest#implementations).\n",
    "\n",
    "### Saving to OME-TIFF\n",
    "\n",
    "The simpliest method to save your image as an OME-TIFF file with key pieces of\n",
    "metadata is to use the `save` function.\n",
    "\n",
    "```python\n",
    "from aicsimageio import AICSImage\n",
    "\n",
    "AICSImage(\"my_file.czi\").save(\"my_file.ome.tiff\")\n",
    "```\n",
    "\n",
    "**Note:** By default `aicsimageio` will generate only a portion of metadata to pass\n",
    "along from the reader to the OME model. This function currently does not do a full\n",
    "metadata translation.\n",
    "\n",
    "For finer grain customization of the metadata, scenes, or if you want to save an array\n",
    "as an OME-TIFF, the writer class can also be used to customize as needed.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from aicsimageio.writers import OmeTiffWriter\n",
    "\n",
    "image = np.random.rand(10, 3, 1024, 2048)\n",
    "OmeTiffWriter.save(image, \"file.ome.tif\", dim_order=\"ZCYX\")\n",
    "```\n",
    "\n",
    "See\n",
    "[OmeTiffWriter documentation](./aicsimageio.writers.html#aicsimageio.writers.ome_tiff_writer.OmeTiffWriter.save)\n",
    "for more details.\n",
    "\n",
    "#### Other Writers\n",
    "\n",
    "In most cases, `AICSImage.save` is usually a good default but there are other image\n",
    "writers available. For more information, please refer to\n",
    "[our writers documentation](https://allencellmodeling.github.io/aicsimageio/aicsimageio.writers.html).\n",
    "\n",
    "## Benchmarks\n",
    "\n",
    "AICSImageIO is benchmarked using [asv](https://asv.readthedocs.io/en/stable/).\n",
    "You can find the benchmark results for every commit to `main` starting at the 4.0\n",
    "release on our\n",
    "[benchmarks page](https://AllenCellModeling.github.io/aicsimageio/_benchmarks/index.html).\n",
    "\n",
    "## Development\n",
    "\n",
    "See our\n",
    "[developer resources](https://allencellmodeling.github.io/aicsimageio/developer_resources)\n",
    "for information related to developing the code.\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you find `aicsimageio` useful, please cite this repository as:\n",
    "\n",
    "> Eva Maxfield Brown, Dan Toloudis, Jamie Sherman, Madison Swain-Bowden, Talley Lambert, AICSImageIO Contributors (2021). AICSImageIO: Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Pure Python [Computer software]. GitHub. https://github.com/AllenCellModeling/aicsimageio\n",
    "\n",
    "bibtex:\n",
    "\n",
    "```bibtex\n",
    "@misc{aicsimageio,\n",
    "  author    = {Brown, Eva Maxfield and Toloudis, Dan and Sherman, Jamie and Swain-Bowden, Madison and Lambert, Talley and {AICSImageIO Contributors}},\n",
    "  title     = {AICSImageIO: Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Pure Python},\n",
    "  year      = {2021},\n",
    "  publisher = {GitHub},\n",
    "  url       = {https://github.com/AllenCellModeling/aicsimageio}\n",
    "}\n",
    "```\n",
    "\n",
    "_Free software: BSD-3-Clause_\n",
    "\n",
    "_(The LIF component is licensed under GPLv3 and is not included in this package)_\n",
    "_(The Bio-Formats component is licensed under GPLv2 and is not included in this package)_\n",
    "_(The CZI component is licensed under GPLv3 and is not included in this package)_\n",
    "\"\"\"\n",
    "\n",
    "CDP_DATA_README = \"\"\"\n",
    "# cdp-data\n",
    "\n",
    "[![Build Status](https://github.com/CouncilDataProject/cdp-data/workflows/CI/badge.svg)](https://github.com/CouncilDataProject/cdp-data/actions)\n",
    "[![Documentation](https://github.com/CouncilDataProject/cdp-data/workflows/Documentation/badge.svg)](https://CouncilDataProject.github.io/cdp-data)\n",
    "\n",
    "Data Utilities and Processing Generalized for All CDP Instances\n",
    "\n",
    "---\n",
    "\n",
    "![Keywords over time in Seattle, Portland, and Oakland](https://raw.githubusercontent.com/CouncilDataProject/cdp-data/main/docs/_static/header-keywords-over-time.png)\n",
    "\n",
    "## Installation\n",
    "\n",
    "**Stable Release:** `pip install cdp-data`<br>\n",
    "**Development Head:** `pip install git+https://github.com/CouncilDataProject/cdp-data.git`\n",
    "\n",
    "## Documentation\n",
    "\n",
    "For full package documentation please visit [councildataproject.github.io/cdp-data](https://councildataproject.github.io/cdp-data).\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "### Pulling Datasets\n",
    "\n",
    "Install basics: `pip install cdp-data`\n",
    "\n",
    "#### Transcripts and Session Data\n",
    "\n",
    "```python\n",
    "from cdp_data import CDPInstances, datasets\n",
    "\n",
    "ds = datasets.get_session_dataset(\n",
    "    infrastructure_slug=CDPInstances.Seattle,\n",
    "    start_datetime=\"2021-01-01\",\n",
    "    store_transcript=True,\n",
    ")\n",
    "```\n",
    "\n",
    "##### Transcript Schema and Usage\n",
    "\n",
    "It may be useful to look at our\n",
    "[transcript model documentation](https://councildataproject.org/cdp-backend/transcript_model.html).\n",
    "\n",
    "Transcripts can be read into memory and processed as an object:\n",
    "\n",
    "```python\n",
    "from cdp_backend.pipeline.transcript_model import Transcript\n",
    "\n",
    "# Read the file as a Transcript object\n",
    "with open(\"transcript.json\", \"r\") as open_f:\n",
    "    transcript = Transcript.from_json(open_f.read())\n",
    "\n",
    "# Navigate the object\n",
    "for sentence in transcript.sentences:\n",
    "    if \"clerk\" in sentence.text.lower():\n",
    "        print(f\"{sentence.index}, {sentence.start_time}: '{sentence.text}')\n",
    "```\n",
    "\n",
    "If you do not want to do this processing in Python or prefer to work with\n",
    "a DataFrame, you can convert transcripts to DataFrames like so:\n",
    "\n",
    "```python\n",
    "from cdp_data import datasets\n",
    "\n",
    "# assume that transcript is the same transcript as the prior code snippet\n",
    "sentences = datasets.convert_transcript_to_dataframe(transcript)\n",
    "```\n",
    "\n",
    "You can also do this conversion (and storage of the coverted transcript) for\n",
    "all transcripts in a session dataset during dataset construction with the\n",
    "`store_transcript_as_csv` parameter.\n",
    "\n",
    "```python\n",
    "from cdp_data import CDPInstances, datasets\n",
    "\n",
    "ds = datasets.get_session_dataset(\n",
    "    infrastructure_slug=CDPInstances.Seattle,\n",
    "    start_datetime=\"2021-01-01\",\n",
    "    store_transcript=True,\n",
    "    store_transcript_as_csv=True,\n",
    ")\n",
    "```\n",
    "\n",
    "This will store the transcript for each session as both JSON and CSV.\n",
    "\n",
    "#### Voting Data\n",
    "\n",
    "```python\n",
    "from cdp_data import CDPInstances, datasets\n",
    "\n",
    "ds = dataset.get_vote_dataset(\n",
    "    infrastructure_slug=CDPInstances.Seattle,\n",
    "    start_datetime=\"2021-01-01\",\n",
    ")\n",
    "```\n",
    "\n",
    "#### Data Definitions and Schema\n",
    "\n",
    "Please refer to our\n",
    "[database schema](https://councildataproject.org/cdp-backend/database_schema.html)\n",
    "and our\n",
    "[database model definitions](https://councildataproject.org/cdp-backend/cdp_backend.database.html#module-cdp_backend.database.models)\n",
    "for more information on CDP generated and archived data is structured.\n",
    "\n",
    "#### Saving Datasets\n",
    "\n",
    "Because we heavily rely on our database models for database interaction,\n",
    "in many cases, we default to returning the full `fireo.models.Model` object\n",
    "as column values.\n",
    "\n",
    "These objects cannot be immediately stored to disk so we provide a helper to\n",
    "replace all model objects with their database IDs for storage.\n",
    "\n",
    "This can be done directly if you already have a dataset you have been working with:\n",
    "\n",
    "```python\n",
    "from cdp_data import datasets\n",
    "\n",
    "# data should be a pandas dataframe\n",
    "dataset.save_dataset(data, \"data.csv\")\n",
    "```\n",
    "\n",
    "Or this can be premptively be done during dataset construction:\n",
    "\n",
    "```python\n",
    "from cdp_data import CDPInstances, dataset\n",
    "\n",
    "# both get_session_dataset and get_vote_dataset\n",
    "# have a `replace_py_objects` parameter\n",
    "sessions = datasets.get_session_dataset(\n",
    "    infrastructure_slug=CDPInstances.Seattle,\n",
    "    replace_py_objects=True,\n",
    ")\n",
    "\n",
    "votes = datasets.get_vote_dataset(\n",
    "    infrastructure_slug=CDPInstances.Seattle,\n",
    "    replace_py_objects=True,\n",
    ")\n",
    "```\n",
    "\n",
    "### Plotting and Analysis\n",
    "\n",
    "Install plotting support: `pip install cdp-data[plot]`\n",
    "\n",
    "#### Ngram Usage over Time\n",
    "\n",
    "```python\n",
    "from cdp_data import CDPInstances, keywords, plots\n",
    "\n",
    "ngram_usage = keywords.compute_ngram_usage_history(\n",
    "    CDPInstances.Seattle,\n",
    "    start_datetime=\"2022-03-01\",\n",
    "    end_datetime=\"2022-10-01\",\n",
    ")\n",
    "grid = plots.plot_ngram_usage_histories(\n",
    "    [\"police\", \"housing\", \"transportation\"],\n",
    "    ngram_usage,\n",
    "    lmplot_kws=dict(  # extra plotting params\n",
    "        col=\"ngram\",\n",
    "        hue=\"ngram\",\n",
    "        scatter_kws={\"alpha\": 0.2},\n",
    "        aspect=1.6,\n",
    "    ),\n",
    ")\n",
    "grid.savefig(\"seattle-keywords-over-time.png\")\n",
    "```\n",
    "\n",
    "![Seattle keyword usage over time](https://raw.githubusercontent.com/CouncilDataProject/cdp-data/main/docs/_static/seattle-keywords-over-time.png)\n",
    "\n",
    "## Development\n",
    "\n",
    "See [CONTRIBUTING.md](CONTRIBUTING.md) for information related to developing the code.\n",
    "\n",
    "**MIT license**\n",
    "\"\"\"\n",
    "\n",
    "CDP_OAKLAND_README = \"\"\"\n",
    "# CDP - Oakland\n",
    "\n",
    "[![Infrastructure Deployment Status](https://github.com/CouncilDataProject/oakland/workflows/Infrastructure/badge.svg)](https://github.com/CouncilDataProject/oakland/actions?query=workflow%3A%22Infrastructure%22)\n",
    "[![Event Processing Pipeline](https://github.com/CouncilDataProject/oakland/workflows/Event%20Gather/badge.svg)](https://github.com/CouncilDataProject/oakland/actions?query=workflow%3A%22Event+Gather%22)\n",
    "[![Event Index Pipeline](https://github.com/CouncilDataProject/oakland/workflows/Event%20Index/badge.svg)](https://github.com/CouncilDataProject/oakland/actions?query=workflow%3A%22Event+Index%22)\n",
    "[![Web Deployment Status](https://github.com/CouncilDataProject/oakland/workflows/Web%20App/badge.svg)](https://councildataproject.github.io/oakland)\n",
    "[![Repo Build Status](https://github.com/CouncilDataProject/oakland/workflows/Build%20Main/badge.svg)](https://github.com/CouncilDataProject/oakland/actions?query=workflow%3A%22Build+Main%22)\n",
    "\n",
    "---\n",
    "\n",
    "## Council Data Project\n",
    "\n",
    "Council Data Project is an open-source project dedicated to providing journalists, activists, researchers, and all members of each community we serve with the tools they need to stay informed and hold their Council Members accountable.\n",
    "\n",
    "For more information about Council Data Project, please visit [our website](https://councildataproject.org/).\n",
    "\n",
    "## Instance Information\n",
    "\n",
    "This repo serves the municipality: **Oakland**\n",
    "\n",
    "### Python Access\n",
    "\n",
    "Install:\n",
    "\n",
    "`pip install cdp-backend`\n",
    "\n",
    "Quickstart:\n",
    "\n",
    "```python\n",
    "from cdp_backend.database import models as db_models\n",
    "from cdp_backend.pipeline.transcript_model import Transcript\n",
    "import fireo\n",
    "from gcsfs import GCSFileSystem\n",
    "from google.auth.credentials import AnonymousCredentials\n",
    "from google.cloud.firestore import Client\n",
    "\n",
    "# Connect to the database\n",
    "fireo.connection(client=Client(\n",
    "    project=\"cdp-oakland-ba81c097\",\n",
    "    credentials=AnonymousCredentials()\n",
    "))\n",
    "\n",
    "# Read from the database\n",
    "five_people = list(db_models.Person.collection.fetch(5))\n",
    "\n",
    "# Connect to the file store\n",
    "fs = GCSFileSystem(project=\"cdp-oakland-ba81c097\", token=\"anon\")\n",
    "\n",
    "# Read a transcript's details from the database\n",
    "transcript_model = list(db_models.Transcript.collection.fetch(1))[0]\n",
    "\n",
    "# Read the transcript directly from the file store\n",
    "with fs.open(transcript_model.file_ref.get().uri, \"r\") as open_resource:\n",
    "    transcript = Transcript.from_json(open_resource.read())\n",
    "\n",
    "# OR download and store the transcript locally with `get`\n",
    "fs.get(transcript_model.file_ref.get().uri, \"local-transcript.json\")\n",
    "# Then read the transcript from your local machine\n",
    "with open(\"local-transcript.json\", \"r\") as open_resource:\n",
    "    transcript = Transcript.from_json(open_resource.read())\n",
    "```\n",
    "\n",
    "-   See the [CDP Database Schema](https://councildataproject.org/cdp-backend/database_schema.html)\n",
    "    for a Council Data Project database schema diagram.\n",
    "-   See the [FireO documentation](https://octabyte.io/FireO/)\n",
    "    to learn how to construct queries using CDP database models.\n",
    "-   See the [GCSFS documentation](https://gcsfs.readthedocs.io/en/latest/index.html)\n",
    "    to learn how to retrieve files from the file store.\n",
    "\n",
    "## Contributing\n",
    "\n",
    "If you wish to contribute to CDP please note that the best method to do so is to contribute to the upstream libraries that compose the CDP Instances themselves. These are detailed below.\n",
    "\n",
    "-   [cdp-backend](https://github.com/CouncilDataProject/cdp-backend): Contains all the database models, data processing pipelines, and infrastructure-as-code for CDP deployments. Contributions here will be available to all CDP Instances. Entirely written in Python.\n",
    "-   [cdp-frontend](https://github.com/CouncilDataProject/cdp-frontend): Contains all of the components used by the web apps to be hosted on GitHub Pages. Contributions here will be available to all CDP Instances. Entirely written in TypeScript and React.\n",
    "-   [cookiecutter-cdp-deployment](https://github.com/CouncilDataProject/cookiecutter-cdp-deployment): The repo used to generate new CDP Instance deployments. Like this repo!\n",
    "-   [councildataproject.org](https://github.com/CouncilDataProject/councildataproject.github.io): Our landing page! Contributions here should largely be text changes and admin updates.\n",
    "\n",
    "## Instance Admin Documentation\n",
    "\n",
    "You can find documentation on how to customize, update, and maintain this CDP instance\n",
    "in the\n",
    "[admin-docs directory](https://github.com/CouncilDataProject/oakland/tree/main/admin-docs).\n",
    "\n",
    "## License\n",
    "\n",
    "CDP software is licensed under a [MIT License](./LICENSE).\n",
    "\n",
    "Content produced by this instance is available under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\"\"\"\n",
    "\n",
    "LAZY_TEXT_CLASSIFIERS_README = \"\"\"\n",
    "# lazy-text-classifiers\n",
    "\n",
    "[![Build Status](https://github.com/evamaxfield/lazy-text-classifiers/workflows/CI/badge.svg)](https://github.com/evamaxfield/lazy-text-classifiers/actions)\n",
    "[![Documentation](https://github.com/evamaxfield/lazy-text-classifiers/workflows/Documentation/badge.svg)](https://evamaxfield.github.io/lazy-text-classifiers)\n",
    "\n",
    "Build and test a variety of text binary or multi-class classification models.\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "**Stable Release:** `pip install lazy-text-classifiers`<br>\n",
    "**Development Head:** `pip install git+https://github.com/evamaxfield/lazy-text-classifiers.git`\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "```python\n",
    "from lazy_text_classifiers import LazyTextClassifiers\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example data from sklearn\n",
    "# `x` should be an iterable of strings\n",
    "# `y` should be an iterable of string labels\n",
    "data = fetch_20newsgroups(subset=\"all\", remove=(\"header\", \"footers\", \"quotes\"))\n",
    "x = data.data[:1000]\n",
    "y = data.target[:1000]\n",
    "y = [data.target_names[id_] for id_ in y]\n",
    "\n",
    "# Split the data into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x,\n",
    "    y,\n",
    "    test_size=0.4,\n",
    "    random_state=12,\n",
    ")\n",
    "\n",
    "# Init and fit all models\n",
    "ltc = LazyTextClassifiers(random_state=12)\n",
    "results = ltc.fit(x_train, x_test, y_train, y_test)\n",
    "\n",
    "# Results is a dataframe\n",
    "# | model                  |   accuracy |   balanced_accuracy |   precision |   recall |       f1 |    time |\n",
    "# |:-----------------------|-----------:|--------------------:|------------:|---------:|---------:|--------:|\n",
    "# | semantic-logit         |    0.73    |            0.725162 |    0.734887 |  0.73    | 0.728247 |  13.742 |\n",
    "# | tfidf-logit            |    0.70625 |            0.700126 |    0.709781 |  0.70625 | 0.702073 | 187.217 |\n",
    "# | fine-tuned-transformer |    0.11125 |            0.1118   |    0.10998  |  0.11125 | 0.109288 | 220.105 |\n",
    "\n",
    "# Get a specific model\n",
    "semantic_logit = ltc.fit_models[\"semantic-logit\"]\n",
    "# either an scikit-learn Pipeline or a custom Transformer wrapper class\n",
    "\n",
    "# All models have a `save` function which will store into the normal format\n",
    "# * pickle for scikit-learn pipelines\n",
    "# * torch model directory for Transformers\n",
    "```\n",
    "\n",
    "## Documentation\n",
    "\n",
    "For full package documentation please visit [evamaxfield.github.io/lazy-text-classifiers](https://evamaxfield.github.io/lazy-text-classifiers).\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This package was heavily inspired by [lazypredict](https://github.com/shankarpandala/lazypredict).\n",
    "\n",
    "## Development\n",
    "\n",
    "See [CONTRIBUTING.md](CONTRIBUTING.md) for information related to developing the code.\n",
    "\n",
    "**MIT License**\n",
    "\"\"\"\n",
    "\n",
    "SPEAKERBOX_README = \"\"\"\n",
    "# speakerbox\n",
    "\n",
    "[![Build Status](https://github.com/CouncilDataProject/speakerbox/workflows/CI/badge.svg)](https://github.com/CouncilDataProject/speakerbox/actions)\n",
    "[![Documentation](https://github.com/CouncilDataProject/speakerbox/workflows/Documentation/badge.svg)](https://CouncilDataProject.github.io/speakerbox)\n",
    "[![status](https://joss.theoj.org/papers/49cfcef1769c812ce4ff2e388a5c7641/status.svg)](https://joss.theoj.org/papers/49cfcef1769c812ce4ff2e388a5c7641)\n",
    "\n",
    "Few-Shot Multi-Recording Speaker Identification Transformer Fine-Tuning and Application\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "**Stable Release:** `pip install speakerbox`<br>\n",
    "**Development Head:** `pip install git+https://github.com/CouncilDataProject/speakerbox.git`\n",
    "\n",
    "## Documentation\n",
    "\n",
    "For full package documentation please visit [councildataproject.github.io/speakerbox](https://councildataproject.github.io/speakerbox).\n",
    "\n",
    "## Example Usage Video\n",
    "\n",
    "[![screenshot from example usage youtube video](https://raw.githubusercontent.com/CouncilDataProject/speakerbox/main/docs/_static/images/speakerbox-example-video-screenshot.png)](https://youtu.be/SK2oVqSKPTE)\n",
    "\n",
    "Link: [https://youtu.be/SK2oVqSKPTE](https://youtu.be/SK2oVqSKPTE)\n",
    "\n",
    "In the example video, we use the Speakerbox library to quickly annotate a \n",
    "dataset of audio clips from the show \n",
    "[The West Wing](https://en.wikipedia.org/wiki/The_West_Wing) \n",
    "and train a speaker identification model to identify three of \n",
    "the show's characters (President Bartlet, Charlie Young, and Leo McGarry).\n",
    "\n",
    "## Problem\n",
    "\n",
    "Given a set of recordings of multi-speaker recordings:\n",
    "\n",
    "```\n",
    "example/\n",
    "├── 0.wav\n",
    "├── 1.wav\n",
    "├── 2.wav\n",
    "├── 3.wav\n",
    "├── 4.wav\n",
    "└── 5.wav\n",
    "```\n",
    "\n",
    "Where each recording has some or all of a set of speakers, for example:\n",
    "\n",
    "-   0.wav -- contains speakers: A, B, C\n",
    "-   1.wav -- contains speakers: A, C\n",
    "-   2.wav -- contains speakers: B, C\n",
    "-   3.wav -- contains speakers: A, B, C\n",
    "-   4.wav -- contains speakers: A, B, C\n",
    "-   5.wav -- contains speakers: A, B, C\n",
    "\n",
    "You want to train a model to classify portions of audio as one of the N known speakers\n",
    "in future recordings not included in your original training set.\n",
    "\n",
    "`f(audio) -> [(start_time, end_time, speaker), (start_time, end_time, speaker), ...]`\n",
    "\n",
    "i.e. `f(audio) -> [(2.4, 10.5, \"A\"), (10.8, 14.1, \"D\"), (14.8, 22.7, \"B\"), ...]`\n",
    "\n",
    "The `speakerbox` library contains methods for both generating datasets for annotation\n",
    "and for utilizing multiple audio annotation schemes to train such a model.\n",
    "\n",
    "![Typical workflow to prepare a speaker identification dataset and fine-tune a new model using tools provided from the Speakerbox library. The user starts with a collection of audio files that include portions speech from the speakers they want to train a model to identify. The `diarize_and_split_audio` function will create a new directory with the same name as the audio file, diarize the audio file, and finally, sort the audio portions produced from diarization into sub-directories within this new directory. The user should then manually rename each of the produced sub-directories to the correct speaker identifier (i.e. the speaker's name or a unique id) and additionally remove any incorrectly diarized or mislabeled portions of audio. Finally, the user can prepare training, evaluation, and testing datasets (via the `expand_labeled_diarized_audio_dir_to_dataset` and `preprocess_dataset` functions) and fine-tune a new speaker identification model (via the `train` function).](https://raw.githubusercontent.com/CouncilDataProject/speakerbox/main/docs/_static/images/workflow.png)\n",
    "\n",
    "The following table shows model performance results as the dataset size increases:\n",
    "\n",
    "| dataset_size   | mean_accuracy   | mean_precision   | mean_recall   | mean_training_duration_seconds   |\n",
    "|:---------------|----------------:|-----------------:|--------------:|---------------------------------:|\n",
    "| 15-minutes     | 0.874 ± 0.029   | 0.881 ± 0.037    | 0.874 ± 0.029 | 101 ± 1                          |\n",
    "| 30-minutes     | 0.929 ± 0.006   | 0.94 ± 0.007     | 0.929 ± 0.006 | 186 ± 3                          |\n",
    "| 60-minutes     | 0.937 ± 0.02    | 0.94 ± 0.017     | 0.937 ± 0.02  | 453 ± 7                          |\n",
    "\n",
    "All results reported are the average of five model training and evaluation trials for each\n",
    "of the different dataset sizes. All models were fine-tuned using an NVIDIA GTX 1070 TI.\n",
    "\n",
    "**Note:** this table can be reproduced in ~1 hour using an NVIDIA GTX 1070 TI by:\n",
    "\n",
    "Installing the example data download dependency:\n",
    "\n",
    "```bash\n",
    "pip install speakerbox[example_data]\n",
    "```\n",
    "\n",
    "Then running the following commands in Python:\n",
    "\n",
    "```python\n",
    "from speakerbox.examples import (\n",
    "    download_preprocessed_example_data,\n",
    "    train_and_eval_all_example_models,\n",
    ")\n",
    "\n",
    "# Download and unpack the preprocessed example data\n",
    "dataset = download_preprocessed_example_data()\n",
    "\n",
    "# Train and eval models with different subsets of the data\n",
    "results = train_and_eval_all_example_models(dataset)\n",
    "```\n",
    "\n",
    "## Workflow\n",
    "\n",
    "### Diarization\n",
    "\n",
    "We quickly generate an annotated dataset by first diarizing (or clustering based\n",
    "on the features of speaker audio) portions of larger audio files and splitting each the\n",
    "of the clusters into their own directories that you can then manually clean up\n",
    "(by removing incorrectly clustered audio segments).\n",
    "\n",
    "#### Notes\n",
    "\n",
    "-   It is recommended to have each larger audio file named with a unique id that\n",
    "    can be used to act as a \"recording id\".\n",
    "-   Diarization time depends on machine resources and make take a long time -- one\n",
    "    potential recommendation is to run a diarization script overnight and clean up the\n",
    "    produced annotations the following day.\n",
    "-   During this process audio will be duplicated in the form of smaller audio clips --\n",
    "    ensure you have enough space on your machine to complete this process before\n",
    "    you begin.\n",
    "-   Clustering accuracy depends on how many speakers there are, how distinct their\n",
    "    voices are, and how much speech is talking over one-another.\n",
    "-   If possible, try to find recordings where speakers have a roughly uniform distribution\n",
    "    of speaking durations.\n",
    "\n",
    "⚠️ To use the diarization portions of `speakerbox` you need to complete the\n",
    "following steps: ⚠️\n",
    "\n",
    "1. Visit [hf.co/pyannote/speaker-diarization](https://hf.co/pyannote/speaker-diarization)\n",
    "   and accept user conditions.\n",
    "2. Visit [hf.co/pyannote/segmentation](https://hf.co/pyannote/segmentation)\n",
    "   and accept user conditions.\n",
    "3. Visit [hf.co/settings/tokens](https://hf.co/settings/tokens) to create an access token\n",
    "   (only if you had to complete 1.)\n",
    "\n",
    "**Diarize a single file:**\n",
    "\n",
    "```python\n",
    "from speakerbox import preprocess\n",
    "\n",
    "# The token can also be provided via the 'HUGGINGFACE_TOKEN` environment variable.\n",
    "diarized_and_split_audio_dir = preprocess.diarize_and_split_audio(\n",
    "    \"0.wav\",\n",
    "    hf_token=\"token-from-hugging-face\",\n",
    ")\n",
    "```\n",
    "\n",
    "**Diarize all files in a directory:**\n",
    "```python\n",
    "from speakerbox import preprocess\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Iterate over all 'wav' format files in a directory called 'data'\n",
    "for audio_file in tqdm(list(Path(\"data\").glob(\"*.wav\"))):\n",
    "    # The token can also be provided via the 'HUGGINGFACE_TOKEN` environment variable.\n",
    "    diarized_and_split_audio_dir = preprocess.diarize_and_split_audio(\n",
    "        audio_file,\n",
    "        # Create a new directory to place all created sub-directories within\n",
    "        storage_dir=f\"diarized-audio/{audio_file.stem}\",\n",
    "        hf_token=\"token-from-hugging-face\",\n",
    "    )\n",
    "```\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "Diarization will produce a directory structure organized by unlabeled speakers with\n",
    "the audio clips that were clustered together.\n",
    "\n",
    "For example, if `\"0.wav\"` had three speakers, the produced directory structure may look\n",
    "like the following tree:\n",
    "\n",
    "```\n",
    "0/\n",
    "├── SPEAKER_00\n",
    "│   ├── 567-12928.wav\n",
    "│   ├── ...\n",
    "│   └── 76192-82901.wav\n",
    "├── SPEAKER_01\n",
    "│   ├── 34123-38918.wav\n",
    "│   ├── ...\n",
    "│   └── 88212-89111.wav\n",
    "└── SPEAKER_02\n",
    "    ├── ...\n",
    "    └── 53998-62821.wav\n",
    "```\n",
    "\n",
    "We leave it to you as a user to then go through these directories and remove any audio\n",
    "clips that were incorrectly clustered together as well as renaming the sub-directories\n",
    "to their correct speaker labels. For example, labelled sub-directories may look like\n",
    "the following tree:\n",
    "\n",
    "```\n",
    "0/\n",
    "├── A\n",
    "│   ├── 567-12928.wav\n",
    "│   ├── ...\n",
    "│   └── 76192-82901.wav\n",
    "├── B\n",
    "│   ├── 34123-38918.wav\n",
    "│   ├── ...\n",
    "│   └── 88212-89111.wav\n",
    "└── D\n",
    "    ├── ...\n",
    "    └── 53998-62821.wav\n",
    "```\n",
    "\n",
    "#### Notes\n",
    "\n",
    "-   Most operating systems have an audio playback application to queue an entire directory\n",
    "    of audio files as a playlist for playback. This makes it easy to listen to a whole\n",
    "    unlabeled sub-directory (i.e. \"SPEAKER_00\") at a time and pause playback and remove\n",
    "    files from the directory which were incorrectly clustered.\n",
    "-   If any clips have overlapping speakers, it is up to you as a user if you want to\n",
    "    remove those clips or keep them and properly label them with the speaker you wish to\n",
    "    associate them with.\n",
    "\n",
    "### Training Preparation\n",
    "\n",
    "Once you have annotated what you think is enough recordings, you can try preparing\n",
    "a dataset for training.\n",
    "\n",
    "The following functions will prepare the audio for training by:\n",
    "\n",
    "1. Finding all labeled audio clips in the provided directories\n",
    "2. Chunk all found audio clips into smaller duration clips _(parametrizable)_\n",
    "3. Check that the provided annotated dataset meets the following conditions:\n",
    "    1. There is enough data such that the training, test, and validation subsets all\n",
    "       contain different recording ids.\n",
    "    2. There is enough data such that the training, test, and validation subsets each\n",
    "       contain all labels present in the whole dataset.\n",
    "\n",
    "#### Notes\n",
    "\n",
    "-   During this process audio will be duplicated in the form of smaller audio clips --\n",
    "    ensure you have enough space on your machine to complete this process before\n",
    "    you begin.\n",
    "-   Directory names are used as recording ids during dataset construction.\n",
    "\n",
    "```python\n",
    "from speakerbox import preprocess\n",
    "\n",
    "dataset = preprocess.expand_labeled_diarized_audio_dir_to_dataset(\n",
    "    labeled_diarized_audio_dir=[\n",
    "        \"0/\",  # The cleaned and checked audio clips for recording id 0\n",
    "        \"1/\",  # ... recording id 1\n",
    "        \"2/\",  # ... recording id 2\n",
    "        \"3/\",  # ... recording id 3\n",
    "        \"4/\",  # ... recording id 4\n",
    "        \"5/\",  # ... recording id 5\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_dict, value_counts = preprocess.prepare_dataset(\n",
    "    dataset,\n",
    "    # good if you have large variation in number of data points for each label\n",
    "    equalize_data_within_splits=True,\n",
    "    # set seed to get a reproducible data split\n",
    "    seed=60,\n",
    ")\n",
    "\n",
    "# You can print the value_counts dataframe to see how many audio clips of each label\n",
    "# (speaker) are present in each data subset.\n",
    "value_counts\n",
    "```\n",
    "\n",
    "### Model Training and Evaluation\n",
    "\n",
    "Once you have your dataset prepared and available, you can provide it directly to the\n",
    "training function to begin training a new model.\n",
    "\n",
    "The `eval_model` function will store a filed called `results.md` with the accuracy,\n",
    "precision, and recall of the model and additionally store a file called\n",
    "`validation-confusion.png` which is a\n",
    "[confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "\n",
    "#### Notes\n",
    "\n",
    "-   The model (and evaluation metrics) will be stored in a new directory called\n",
    "    `trained-speakerbox` _(parametrizable)_.\n",
    "-   Training time depends on how much data you have annotated and provided.\n",
    "-   It is recommended to train with an NVidia GPU with CUDA available to speed up\n",
    "    the training process.\n",
    "-   Speakerbox has only been tested on English-language audio and the base model for\n",
    "    fine-tuning was trained on English-language audio. We provide no guarantees as to\n",
    "    it's effectiveness on non-English-language audio. If you try Speakerbox on with\n",
    "    non-English-language audio, please let us know!\n",
    "\n",
    "```python\n",
    "from speakerbox import train, eval_model\n",
    "\n",
    "# dataset_dict comes from previous preparation step\n",
    "train(dataset_dict)\n",
    "\n",
    "eval_model(dataset_dict[\"valid\"])\n",
    "```\n",
    "\n",
    "## Model Inference\n",
    "\n",
    "Once you have a trained model, you can use it against a new audio file:\n",
    "\n",
    "```python\n",
    "from speakerbox import apply\n",
    "\n",
    "annotation = apply(\n",
    "    \"new-audio.wav\",\n",
    "    \"path-to-my-model-directory/\",\n",
    ")\n",
    "```\n",
    "\n",
    "The apply function returns a\n",
    "[pyannote.core.Annotation](http://pyannote.github.io/pyannote-core/structure.html#annotation).\n",
    "\n",
    "## Development\n",
    "\n",
    "See [CONTRIBUTING.md](CONTRIBUTING.md) for information related to developing the code.\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@article{Brown2023,\n",
    "    doi = {10.21105/joss.05132},\n",
    "    url = {https://doi.org/10.21105/joss.05132},\n",
    "    year = {2023},\n",
    "    publisher = {The Open Journal},\n",
    "    volume = {8},\n",
    "    number = {83},\n",
    "    pages = {5132},\n",
    "    author = {Eva Maxfield Brown and To Huynh and Nicholas Weber},\n",
    "    title = {Speakerbox: Few-Shot Learning for Speaker Identification with Transformers},\n",
    "    journal = {Journal of Open Source Software}\n",
    "} \n",
    "```\n",
    "\n",
    "**MIT License**\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1f8276-dcbd-45e0-b571-64bed476b247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEMANTIC RESULTS\n",
      "'soft-search': software-not-predicted\n",
      "'aicsimageio': software-not-predicted\n",
      "'cdp-data': software-not-predicted\n",
      "'cdp-oakland': software-predicted\n",
      "'speakerbox': software-not-predicted\n",
      "'lazy-text-classifiers': software-not-predicted\n",
      "TFIDF RESULTS\n",
      "'soft-search': software-not-predicted\n",
      "'aicsimageio': software-predicted\n",
      "'cdp-data': software-not-predicted\n",
      "'cdp-oakland': software-predicted\n",
      "'speakerbox': software-predicted\n",
      "'lazy-text-classifiers': software-predicted\n"
     ]
    }
   ],
   "source": [
    "READMES = {\n",
    "    \"soft-search\": SOFT_SEARCH_README,\n",
    "    \"aicsimageio\": AICSIMAGEIO_README, \n",
    "    \"cdp-data\": CDP_DATA_README, \n",
    "    \"cdp-oakland\": CDP_OAKLAND_README, \n",
    "    \"speakerbox\": SPEAKERBOX_README, \n",
    "    \"lazy-text-classifiers\": LAZY_TEXT_CLASSIFIERS_README,\n",
    "}\n",
    "\n",
    "# Predict\n",
    "semantic_results = semantic_logit.predict(list(READMES.values()))\n",
    "tfidf_results = tfidf_logit.predict(list(READMES.values()))\n",
    "\n",
    "print(\"SEMANTIC RESULTS\")\n",
    "for short_name, result in zip(list(READMES.keys()), semantic_results):\n",
    "    print(f\"'{short_name}': {result}\")\n",
    "\n",
    "print(\"TFIDF RESULTS\")\n",
    "for short_name, result in zip(list(READMES.keys()), tfidf_results):\n",
    "    print(f\"'{short_name}': {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32254b8-a37b-4f87-bfcb-64ff76e578f7",
   "metadata": {},
   "source": [
    "## Trying To Understand Why It's Bad\n",
    "\n",
    "`eli5` has a problem with scikit-learn 1.3.0: https://github.com/TeamHG-Memex/eli5/issues/425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7790d-a1fa-4681-9a18-4f8c4e89a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"scikit-learn>=1,<1.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3623bacd-69c8-4723-91fc-24d1e762e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(ngram_range=(1, 2), stop_words='english',\n",
      "                strip_accents='unicode')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=software-predicted\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +2.182\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        package\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 85.68%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.355\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        installation\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.46%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.250\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        version\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.59%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.233\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        gpu\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.03%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.175\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        install\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.06%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.044\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        video\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.15%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.033\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        examples\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.43%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.998\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        library\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.53%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.987\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        example\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.65%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.972\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        tool\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.82%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.951\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        run\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.94%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.937\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        docker\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.14%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.913\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        phcpack\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.24%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.900\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        arsenic\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.38%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.883\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        software\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.49%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.871\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        algorithms\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.52%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.867\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        development\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.55%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.864\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        xmdvtool\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.57%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.861\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        systems\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.64%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.853\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        py\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.65%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.851\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        lider\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.70%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.846\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        build\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.75%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.840\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        application\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.90%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.822\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        documentation\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.11%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.798\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        features\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.11%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 68662 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 90.10%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 55636 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 90.10%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.799\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        hydroshare\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 89.67%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.849\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        access\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 89.63%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.854\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        moltemplate\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 89.32%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.890\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        course\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 89.26%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.898\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        analysis\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 89.19%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.906\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        oac\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.42%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.000\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        national science\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.42%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.000\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        science foundation\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.21%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.026\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        foundation\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 88.01%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.050\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        api\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.93%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.188\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        science\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.74%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.214\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        workshop\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 86.56%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.236\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        national\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 83.69%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.630\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        dataset\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 82.25%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.840\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        data\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "\n",
    "eli5.show_weights(tfidf_logit, top=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eb0c01-0a68-409b-a524-3805fb8b4872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
