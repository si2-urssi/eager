---
title: "Searching for Software in NSF Awards"
author:
  - name: "Eva Maxfield Brown"
    orcid: 0000-0003-2564-0373
    affliations:
      - name: University of Washington Information School
  - name: "Lindsey Schwartz"
    orcid: XXXX-XXXX-XXXX-XXXX
    affliations:
      - name: University of Washington Information School
  - name: "Richard Lewei Huang"
    orcid: XXXX-XXXX-XXXX-XXXX
    affliations:
      - name: University of Washington Information School
  - name: "Nicholas Weber"
    orcid: XXXX-XXXX-XXXX-XXXX
    email: nmweber@uw.edu
    affliations:
      - name: University of Washington Information School

abstract: "Some abstract!"

# Styling
# See Quarto docs for more details on styling
# This link is for styling options specific to HTML outputs
# https://quarto.org/docs/reference/formats/html.html

## Basics
bibliography: support/main.bib

## Citation Style Language
# See https://github.com/citation-style-language/styles for more options
# We default to PNAS (Proceedings of the National Academy of Sciences)
csl: support/acm-proceedings.csl

## Specific for target format
format:
  html:
    code-tools: true
    standalone: true
    embed-resources: true
    toc: true
    toc-location: left
    reference-location: margin
    citation-location: margin

execute:
  echo: false
---

## Introduction

Some content!

## Data Collection and Annotation {#sec-data-collection}

### Finding Software Produced by NSF Awards {#sec-finding-soft}

The first step in our data collection process was to find software outputs from National Science Foundation (NSF) funded research. Our first attempt at finding such software outputs was to search for references and promises of software production within NSF award abstracts, project outcome reports, and papers supported by each award. This initial search process had two main problems: it was time consuming to conduct and it was difficult to verify claims of software production.

```{python}
#| code-fold: true
#| warning: false

from soft_search.data.soft_search_2022 import load_github_repos_with_nsf_refs_2022
from IPython.display import Markdown

github_repos_with_nsf_refs = load_github_repos_with_nsf_refs_2022()
github_query_terms = github_repos_with_nsf_refs["query"].unique()
github_query_terms_str = ""
for i, term in enumerate(github_query_terms):
  if i == 0:
    github_query_terms_str = f"'{term}'"
  elif i < len(github_query_terms) - 1:
    github_query_terms_str = f"{github_query_terms_str}, '{term}'"
  else:
    github_query_terms_str = f"{github_query_terms_str}, or '{term}'"

Markdown(
  f"Our solution to these issues was to instead create a Python script which used "
  f"the GitHub API to search for repositories which included reference to financial "
  f"support from an NSF award in the repositories README.md file. Specifically "
  f"our script queried for README.md files which contained any of the following "
  f"text snippets: {github_query_terms_str}. "
  f"GitHub was selected as the basis for our search because of it's widespread "
  f"adoption and mention in scholarly publication "
  f"[@riseofgithubinscholarlypublication]. "
  f"This search found {len(github_repos_with_nsf_refs)} "
  f"unique repositories which contained a reference to the NSF in the repository's "
  f"README.md file."
)
```

### Software Production Annotation

The next step in our data collection process was to annotate each of the GitHub repositories found as either "software" or "not software." In our initial review of the repositories we had collected, we found that the content of repositories ranged from documentation, experimental notes, course materials, collections of one-off scripts written during a research project, to more typical software libraries with installation instructions, testing, and community support and use.

To come to agreement on annotation criteria we conducted multiple rounds of trial coding on samples of the data. Fleiss' kappa was used to determine if there was agreement between our research team on whether ten GitHub repositories contained 'software' or not. On each round of trial coding ten GitHub repositories were randomly selected from our dataset for each member of our research team to annotate independently. When assessing a repository, members of the research team were allowed to use any information in the repository to determine their annotation (i.e. the content of the README.md file, the repository activity, documentation availability, etc.)

```{python}
#| code-fold: true
#| warning: false

from soft_search.data.irr import print_irr_summary_stats

irr_kappa = print_irr_summary_stats(do_print=False)

Markdown(
  f"Our final round of trial coding showed that there was almost perfect agreement "
  f"between the research team (K={round(irr_kappa, 3)})."
)
```

Our final annotation criteria was generally inclusive of labeling repositories as software, rather there were specific exclusion criteria that resulted in a repository being labeled as "not software". Specifically repositories were labeled as "not software" when a repository primarily consisted of:

1. project documentation or research notes
2. teaching materials for a workshop or course
3. the source code for a project or research lab website
4. collections of scripts specific to the analysis of a single experiment without regard to futher generalizability
5. utility functions for accessing data without providing any additional processing capacity

We then annotated all GitHub repositories in our dataset as either "software" or "not software" according to our agreed upon annotation criteria.

### Linking GitHub Repositories to NSF Awards

Our final step in the data collection process was to link the annotated GitHub repositories back to specific NSF awards. To do so, we created a script which would load the webpage for each GitHub repository, scrape the content of the README and find the specific NSF award ID number(s) referenced. While annotating the dataset, and with this script, our dataset size was reduced as we found some repositories were returned in the initial search because of the "NSF" acronym being used by other, non-United-States governmental agencies which also fund research. 

When processing each repository, our Python script would load the README content, search for NSF Award ID patterns with regular expressions, and then verify that each NSF award ID found was a valid NSF award ID by requesting metadata for the award from the NSF award API.

```{python}
#| code-fold: true
#| warning: false

from soft_search.data import load_soft_search_2022
from soft_search.data.soft_search_2022 import SoftSearch2022DatasetFields
from soft_search.constants import PredictionLabels

soft_search_2022 = load_soft_search_2022()
num_awards_produced_software = len(
  soft_search_2022[
    soft_search_2022[SoftSearch2022DatasetFields.label] == (
      PredictionLabels.SoftwarePredicted
    )
  ]
)
num_awards_did_not_produce_software = len(
  soft_search_2022[
    soft_search_2022[SoftSearch2022DatasetFields.label] == (
      PredictionLabels.SoftwareNotPredicted
    )
  ]
)

Markdown(
  f"During this NSF award ID validation process we additionally completed the "
  f"final step for constructing our dataset which was to request the text for "
  f"the award's abstract and project outcomes report. This was the final step "
  f"of our data collection process and allowed us to create a dataset of "
  f"{num_awards_produced_software} unique NSF awards labeled as 'produced software' "
  f"and {num_awards_did_not_produce_software} unique NSF awards labeled as "
  f"'did not produce software'."
)
```

## Predictive Models

We trained three different models using the text from either the award abstract or project outcomes report. The models trained include a logistic regression model trained with TF-IDF word embeddings (`tfidf-logit`), a logistic regression model trained with semantic embeddings (`semantic-logit`), and a fine-tuned transformer (`fine-tuned-transformer`). The semantic embeddings and the base model from which we fine-tuned our own transformer model was the `distilbert-base-uncased-finetuned-sst-2-english` model [@hf_canonical_model_maintainers_2022]. Additionally, we evaluated a regular expression which matched on common terms relating to software as a baseline. Each model was trained with 80% of the data. We then test each of the models and use F1 to rank each models performance.

```{python}
#| code-fold: true
#| warning: false
#| echo: false
#| output: false

from soft_search.label.model_selection import fit_and_eval_all_models

model_results = fit_and_eval_all_models(train_transformer=False)
```

```{python}
#| code-fold: true
#| warning: false
#| label: tbl-model-results-from-abstract
#| tbl-cap: Predictive Model Results (Trained with Abstract Text)

from_abstract = model_results[
  model_results["predictive_source"] == "abstract-text"
].reset_index(drop=True)

from_abstract
```

```{python}
#| code-fold: true
#| warning: false

best_model_from_abstract = from_abstract.iloc[0]
best_model_from_abstract_name = best_model_from_abstract["model"]
best_model_from_abstract_f1 = best_model_from_abstract["f1"]

Markdown(
  f"@tbl-model-results-from-abstract reports the results from training using "
  f"the abstract text as input. The best performing model was the "
  f"`{best_model_from_abstract_name}` which achieved an F1 of "
  f"{round(best_model_from_abstract_f1, 3)}."
)
```

```{python}
#| code-fold: true
#| warning: false
#| label: tbl-model-results-from-project-outcomes
#| tbl-cap: Predictive Model Results (Trained with Project Outcomes Report Text)

from_po = model_results[
  model_results["predictive_source"] == "project-outcomes"
].reset_index(drop=True)

from_po
```

```{python}
#| code-fold: true
#| warning: false

best_model_from_po = from_po.iloc[0]
best_model_from_po_name = best_model_from_po["model"]
best_model_from_po_f1 = best_model_from_po["f1"]

Markdown(
  f"@tbl-model-results-from-project-outcomes reports the results from training using "
  f"the project outcomes reports as input. The best performing model was the "
  f"`{best_model_from_po_name}` which achieved an F1 of "
  f"{round(best_model_from_po_f1, 3)}."
)
```

While the models trained with the project outcomes reports were trained with less data, the best model of the group achieved a higher F1 than any of the models trained with the abstracts. While we have not investigated further, we believe this to be because the project outcomes reports contain more direct citation of produced software rather than an abstract's promise of software production.

The data used for training, and functions to reproduce these models, are made available via our Python package: [`soft-search`](https://github.com/si2-urssi/eager).

## The NSF-Soft-Search Dataset

We have compiled the NSF Soft(ware) Search Dataset (`NSF-Soft-Search`). The NSF-Soft-Search dataset contains the metadata, abstract text, project outcomes report text, for all NSF awarded projects during the 2010-2023 period. The NSF-Soft-Search dataset additionally contains our predictions for software production using both texts respectively. 

```{python}
#| code-fold: true
#| warning: false
#| label: tbl-nsf-soft-search-stats
#| tbl-cap: Composition of the NSF Soft Search Dataset

import pandas as pd

full_dataset = pd.read_parquet("nsf-soft-search-2022.parquet")

# Get descriptive statistics for each major program
desc_table_rows = []
for program in full_dataset.majorProgram.unique():
    subset = full_dataset[full_dataset.majorProgram == program]
    total_awards_funded = len(subset)
    will_produce_software = subset[
      subset.prediction_from_abstract == "software-predicted"
    ]
    num_awards_will_produce_software = len(will_produce_software)
    percent_awards_will_produce = num_awards_will_produce_software / total_awards_funded
    
    desc_table_rows.append({
        "Funding Program": program,
        "# Awards Funded": total_awards_funded,
        "# Awards 'Produce Software'": num_awards_will_produce_software,
        "% Awards 'Produce Software'": percent_awards_will_produce,
    })

desc_table = pd.DataFrame(desc_table_rows).sort_values(
    by=["# Awards Funded", "% Awards 'Produce Software'"],
    ascending=False,
).reset_index(drop=True)
desc_table
```

### Trends and Observations

* Trends in software production over time from abstract
```{python}
#| code-fold: true
#| warning: false

# Create start year, experation year, and award duration columns for downstream
full_dataset["startYear"] = full_dataset.startDate.apply(lambda d: d.split("/")[-1])
full_dataset["expYear"] = full_dataset.expDate.apply(lambda d: d.split("/")[-1])
full_dataset["awardDuration"] = full_dataset.apply(
  lambda r: int(r.expYear) - int(r.startYear),
  axis=1,
)

# Subset the dataset to remove programs which either do not have many awards in total
# or do not produce awards which result in software production
high_soft_prog_df = full_dataset[~full_dataset.majorProgram.isin(
  ["EHR", "SBE", "TIP", "OISE", "OIA"]
)]
```

```{python}
#| code-fold: true
#| warning: false
#| label: fig-soft-over-time
#| fig-cap: Software Production Over Time (Using Predictions from Abstracts)

import seaborn as sns
import matplotlib.pyplot as plt
sns.set_context("paper")
sns.set_theme(
    style="ticks", rc={"axes.spines.right": False, "axes.spines.top": False}
)
sns.set_style("darkgrid", {"grid.color": "#000000", "grid.linestyle": ":"})
sns.set_palette(
    [
        "#ff6a75",  # cherry-red
        "#0060df",  # ocean-blue
        "#068989",  # moss-green
        "#712290",  # purple
        "#FFA537",  # orange
        "#FF2A8A",  # pink
        "#9059FF",  # lavender
        "#00B3F5",  # light-blue / sky-blue
        "#005e5e",  # dark blueish green
        "#C50143",  # dark-red / maroon
        "#3fe1b0",  # seafoam / mint
        "#063F96",  # dark-blue / navy-blue
        "#FFD567",  # banana-yellow
    ]
)

# Prep data for plotting software production over time
soft_produced_over_time_rows = []
for start_year in high_soft_prog_df.startYear.unique():
    year_data = high_soft_prog_df[high_soft_prog_df.startYear == start_year]
    for program_name, award_count in year_data.majorProgram.value_counts().items():
        program_preds = year_data[
          year_data.majorProgram == program_name
        ].prediction_from_abstract.value_counts()
        try:
            predicted_count = program_preds.loc["software-predicted"]
        except KeyError:
            predicted_count = 0

        soft_produced_over_time_rows.append({
            "Funding Program": program_name,
            "Award Start Year": start_year,
            "% Awards 'Produce Software'": predicted_count / award_count,
        })

soft_produced_over_time = pd.DataFrame(soft_produced_over_time_rows)

# Plot over time
sns.barplot(
    data=soft_produced_over_time,
    x="Award Start Year",
    y="% Awards 'Produce Software'",
    hue="Funding Program",
    order=[
        str(i)
        for i in range(
            high_soft_prog_df.startYear.astype(int).min(),
            high_soft_prog_df.startYear.astype(int).max(),
        )
    ],
)
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
```

```{python}
#| code-fold: true
#| warning: false
#| label: fig-soft-over-duration
#| fig-cap: Software Production Grouped By Award Duration (Using Predictions from Abstracts)

# Prep data for plotting software production over time
soft_produced_over_duration_rows = []
for award_duration in high_soft_prog_df.awardDuration.unique():
    duration_data = high_soft_prog_df[high_soft_prog_df.awardDuration == award_duration]
    for program_name, award_count in duration_data.majorProgram.value_counts().items():
        program_preds = duration_data[
          duration_data.majorProgram == program_name
        ].prediction_from_abstract.value_counts()
        try:
            predicted_count = program_preds.loc["software-predicted"]
        except KeyError:
            predicted_count = 0

        soft_produced_over_duration_rows.append({
            "Funding Program": program_name,
            "Award Duration (Years)": award_duration,
            "% Awards 'Produce Software'": predicted_count / award_count,
        })

soft_produced_over_duration = pd.DataFrame(soft_produced_over_duration_rows)

# Plot over time
sns.barplot(
    data=soft_produced_over_duration,
    x="Award Duration (Years)",
    y="% Awards 'Produce Software'",
    hue="Funding Program",
)
plt.tight_layout()
```

* Number of examples that initially were predicted as producing software but didn't over time
* Number of examples that initially were not predicted to produce software but did over time

## Conclusion

We introduce NSF-Soft-Search, a pair of novel datasets for studying software production from NSF funded projects. The NSF-Soft-Search GitHub Annotations dataset is used to train models which aim to predict software production from either the NSF award abstract text or the project outcomes report text. We used these models to generate the NSF-Soft-Search Inferred dataset. The NSF-Soft-Search Inferred dataset includes project metadata, abstract, project outcomes report, and a predictions of software production for each NSF funded project between 2010 and 2023. We hope that NSF-Soft-Search helps further new studies and findings in understanding the role software development plays in scholarly publication.

All datasets and predictive models produced by this work are available from our GitHub repository: [`si2-urssi/eager`](https://github.com/si2-urssi/eager).

### Limitations

As discussed in @sec-data-collection, the NSF-Soft-Search GitHub Annotations dataset was entirely composed of NSF awards which ultimately released or hosted software (and other research products) on GitHub. Due to data collection strategy, it is possible that each of the predictive models learned not to predict if an NSF award would produce software, but rather, if an NSF award would produce “software hosted on GitHub.”

### Future Work

As discussed in @sec-finding-soft, our initial method for attempting to find research software produced from NSF supported awards was to search for references and promises of software production in the abstract, project outcomes report, and attached papers of each award. While attempting this approach to create the dataset, we found that many awards and papers that reference computational methods do not provide a reference web link to their code repositories or websites. In some cases, we found repositories related to an award or paper via Google and GitHub search ourselves. While we support including references to code repositories in award abstracts, outcomes reports, and papers, future research should be conducted on how to enable reconnection of papers with code.

## Acknowledgements

We wish to thank ....

This work was supported in part by NSF Grant

::: {.content-visible unless-format="html"}
## References
:::
