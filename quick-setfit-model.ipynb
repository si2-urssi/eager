{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a411dc6-b3a1-4be2-b125-96b84d47188d",
   "metadata": {},
   "source": [
    "# Training Toy SetFit Models for NSF Award Abstract Software Prediction\n",
    "\n",
    "Quick notebook which uses only a sample of our data, merged in the current annotations from Lindsey and Richard, gets the NSF award abstract texts and then trains a model with SetFit.\n",
    "\n",
    "Larger example to come soon^tm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e21968-5001-4e49-9372-ed9b11e7a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7563a-1d6b-4b82-86f6-e4182c17cec2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1. Read a sample of the \"NSF + GitHub Linked\" data (output from Eva's script)\n",
    "2. Read Lindsey's labelled GitHub Repos for Software Classification\n",
    "3. Read Richard's labelled GitHub Repos for Software Classification\n",
    "4. Join the datasets together and drop any NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714bc287-861d-4e9a-98ef-aa22e8a8196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read nsf + github linked sample\n",
    "linked_nsf_github_sample = pd.read_parquet(\n",
    "    \"/Users/eva/Downloads/linked-github-nsf-results.parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff5eb7-838c-4986-a6ad-adae6ee37fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read lindseys labelled github repos data and clean\n",
    "lindsey_coded_repos = pd.read_csv(\n",
    "    \"/Users/eva/Downloads/all-github-search-results-duplicates-removed - Lindsey.csv\",\n",
    ")\n",
    "lindsey_coded_repos = lindsey_coded_repos[[\"include/exclude\", \"link\"]]\n",
    "lindsey_coded_repos[\"annotator\"] = \"lindsey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705cca6-3072-4614-bc26-cb9431513279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read richards labelled github repos data and clean\n",
    "richard_coded_repos = pd.read_csv(\n",
    "    \"/Users/eva/Downloads/all-github-search-results-duplicates-removed - Richard.csv\",\n",
    ")\n",
    "richard_coded_repos = richard_coded_repos[[\"include/exclude\", \"link\"]]\n",
    "richard_coded_repos[\"annotator\"] = \"richard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54ecef-0b20-4ab2-9ce7-056fdecb726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join and clean\n",
    "data_lindsey = linked_nsf_github_sample.join(\n",
    "    lindsey_coded_repos.set_index(\"link\"), on=\"github_link\",\n",
    ")\n",
    "data_richard = linked_nsf_github_sample.join(\n",
    "    richard_coded_repos.set_index(\"link\"), on=\"github_link\",\n",
    ")\n",
    "data = pd.concat([data_lindsey, data_richard])\n",
    "data = data.dropna(\n",
    "    subset=[\"include/exclude\"],\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994e03d-6de1-4504-8662-1531933ef900",
   "metadata": {},
   "source": [
    "## Quick Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47160a3-48b8-4ef4-8d02-30dcf24cfd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[\n",
    "    data.annotator == \"lindsey\"\n",
    "][\"include/exclude\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3625c5-bad1-44a3-8851-1e3de6dc68bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[\n",
    "    data.annotator == \"richard\"\n",
    "][\"include/exclude\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40769ad0-1fa5-41fb-9e30-1b2721a8ba08",
   "metadata": {},
   "source": [
    "## Get NSF Award Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95926c1-5585-4ee2-a898-aef593ee02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Union\n",
    "\n",
    "import requests\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "from soft_search.constants import NSFFields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed0896-e029-4c05-80d2-63d39b6b5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_abstract_text(award_id: int) -> Dict[str, Union[int, str]]:\n",
    "    return {\n",
    "        \"award_id\": award_id,\n",
    "        \"abstract_text\": requests.get(\n",
    "            f\"https://api.nsf.gov/\"\n",
    "            f\"services/v1/awards/{award_id}.json\"\n",
    "            f\"?printFields={NSFFields.abstractText}\"\n",
    "        ).json()[\"response\"][\"award\"][0][NSFFields.abstractText]\n",
    "    }\n",
    "\n",
    "abstract_texts = pd.DataFrame(\n",
    "    thread_map(\n",
    "        _get_abstract_text,\n",
    "        data.nsf_award_id.unique(),\n",
    "    )\n",
    ")\n",
    "\n",
    "data = data.join(abstract_texts.set_index(\"award_id\"), on=\"nsf_award_id\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06748ed-150c-49f6-be55-63b5d97d85e9",
   "metadata": {},
   "source": [
    "## Prep Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e7f8fd-bff5-468a-b45a-69fed8d1ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99769a-cab8-416d-b2c5-b452d45c4be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data splits of train=0.6 test=0.2 valid=0.2\n",
    "\n",
    "# select only the columns we need\n",
    "subset_data = data[[\"annotator\", \"abstract_text\", \"include/exclude\"]]\n",
    "\n",
    "# lindsey\n",
    "lindsey_data = subset_data.loc[subset_data.annotator == \"lindsey\"].drop(\n",
    "    columns=[\"annotator\"]\n",
    ")\n",
    "lindsey_train, lindsey_test_and_valid = train_test_split(\n",
    "    lindsey_data,\n",
    "    test_size=0.6,\n",
    "    stratify=lindsey_data[\"include/exclude\"],\n",
    ")\n",
    "lindsey_test, lindsey_valid = train_test_split(\n",
    "    lindsey_test_and_valid,\n",
    "    test_size=0.5,\n",
    "    stratify=lindsey_test_and_valid[\"include/exclude\"],\n",
    ")\n",
    "\n",
    "# richard\n",
    "richard_data = subset_data.loc[subset_data.annotator == \"richard\"].drop(\n",
    "    columns=[\"annotator\"]\n",
    ")\n",
    "richard_train, richard_test_and_valid = train_test_split(\n",
    "    richard_data,\n",
    "    test_size=0.6,\n",
    "    stratify=richard_data[\"include/exclude\"],\n",
    ")\n",
    "richard_test, richard_valid = train_test_split(\n",
    "    richard_test_and_valid,\n",
    "    test_size=0.5,\n",
    "    stratify=richard_test_and_valid[\"include/exclude\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9aa0a-ca09-479e-9bf7-1ee1cf7c42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Huggingface Dataset objects\n",
    "lindsey_train = Dataset.from_pandas(lindsey_train, preserve_index=False)\n",
    "lindsey_test = Dataset.from_pandas(lindsey_test, preserve_index=False)\n",
    "lindsey_valid = Dataset.from_pandas(lindsey_valid, preserve_index=False)\n",
    "richard_train = Dataset.from_pandas(richard_train, preserve_index=False)\n",
    "richard_test = Dataset.from_pandas(richard_test, preserve_index=False)\n",
    "richard_valid = Dataset.from_pandas(richard_valid, preserve_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64390711-b611-4bb9-a0dd-99797837984d",
   "metadata": {},
   "source": [
    "## Train Models for Each Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872c2e5-aac3-4ead-9297-55e4e397cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitModel, SetFitTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f84e2-7df4-4d56-8f9f-08848ce88ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a SetFit model from Hub\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402801b-d0b7-4881-acdd-eb56bf48e66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_ds, test_ds, valid_ds in [\n",
    "    (lindsey_train, lindsey_test, lindsey_valid),\n",
    "    (richard_train, lindsey_test, lindsey_valid),\n",
    "]:  \n",
    "    # Create trainer\n",
    "    trainer = SetFitTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=test_ds,\n",
    "        loss_class=CosineSimilarityLoss,\n",
    "        metric=\"accuracy\",\n",
    "        batch_size=16,\n",
    "        num_iterations=20,\n",
    "        num_epochs=1,\n",
    "        column_mapping={\"abstract_text\": \"text\", \"include/exclude\": \"label\"},\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    # trainer.train()\n",
    "    # metrics = trainer.evaluate()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6e4d1-6646-423a-ad39-dca28ec2a13b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
